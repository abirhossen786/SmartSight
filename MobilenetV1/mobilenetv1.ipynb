{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8fb651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c247ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import zipfile\n",
    "from tensorflow.keras import   datasets\n",
    "from tensorflow.keras.optimizers     import   SGD, Adam\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62fd623-c385-43d5-9e2b-c8023a5eda78",
   "metadata": {},
   "outputs": [],
   "source": [
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "annotate = tfmot.quantization.keras.quantize_annotate_layer\n",
    "\n",
    "\n",
    "class ModifiedDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return [(layer.kernel, LastValueQuantizer(num_bits=16, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return [(layer.activation, MovingAverageQuantizer(num_bits=16, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "        layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "        layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45db66e2-77c8-40ed-9e11-7b9faf2897c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class activationquant(tfmot.quantization.keras.QuantizeConfig):\n",
    "    \n",
    "    def _assert_activation_layer(self, layer):\n",
    "        if not isinstance(layer, tf.keras.layers.Activation):\n",
    "            raise RuntimeError(\n",
    "          'Default8BitActivationQuantizeConfig can only be used with '\n",
    "          '`keras.layers.Activation`.')\n",
    "\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        self._assert_activation_layer(layer)\n",
    "        return []\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        self._assert_activation_layer(layer)\n",
    "        return []\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        self._assert_activation_layer(layer)\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        self._assert_activation_layer(layer)\n",
    "\n",
    "    def get_output_quantizers(self, layer):\n",
    "        self._assert_activation_layer(layer)\n",
    "\n",
    "        if not hasattr(layer.activation, '__name__'):\n",
    "            raise ValueError('Activation {} not supported by '\n",
    "                       'Default8BitActivationQuantizeConfig.'.format(\n",
    "                           layer.activation))\n",
    "\n",
    "        if layer.activation.__name__ in ['relu', 'swish']:\n",
    "          # 'relu' should generally get fused into the previous layer.\n",
    "          return [tfmot.quantization.keras.quantizers.MovingAverageQuantizer(\n",
    "              num_bits=16, per_axis=False, symmetric=False, narrow_range=False)]\n",
    "        elif layer.activation.__name__ in [\n",
    "            'linear', 'softmax', 'sigmoid', 'tanh']:\n",
    "            return []\n",
    "\n",
    "        raise ValueError('Activation {} not supported by '\n",
    "                         'Default8BitActivationQuantizeConfig.'.format(\n",
    "                             layer.activation))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4658d0e",
   "metadata": {},
   "source": [
    "## MobilenetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ef65d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for dephwise conv issue: https://stackoverflow.com/questions/48121702/float-ops-found-in-quantized-tensorflow-mobilenet-model\n",
    "\n",
    "\n",
    "l2 = tf.keras.regularizers.L2\n",
    "\n",
    "def mobilenet_v1():\n",
    "    # Mobilenet parameters\n",
    "    input_shape = [96,96,3] # resized to 96x96 per EEMBC requirement\n",
    "    num_classes = 2 # person and non-person\n",
    "    num_filters = 8 # normally 32, but running with alpha=.25 per EEMBC requirement\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs # Keras model uses ZeroPadding2D()\n",
    "\n",
    "    # 1st layer, pure conv\n",
    "    # Keras 2.2 model has padding='valid' and disables bias\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x) # Keras uses ReLU6 instead of pure ReLU\n",
    "\n",
    "    # 2nd layer, depthwise separable conv\n",
    "    # Filter size is always doubled before the pointwise conv\n",
    "    # Keras uses ZeroPadding2D() and padding='valid'\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 3rd layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 4th layer, depthwise separable conv\n",
    "    x = (DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 5th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 6th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 7th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 8th-12th layers, identical depthwise separable convs\n",
    "    # 8th\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 9th\n",
    "    x = annotate(DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'), quantize_config = activationquant())(x)\n",
    "\n",
    "    x = annotate(Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)), quantize_config = ModifiedDenseQuantizeConfig())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'), quantize_config = activationquant())(x)\n",
    "\n",
    "    # 10th\n",
    "    x = annotate(DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'), quantize_config = activationquant())(x)\n",
    "\n",
    "    x = annotate(Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)), quantize_config = ModifiedDenseQuantizeConfig())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'), quantize_config = activationquant())(x)\n",
    "\n",
    "    # 11th\n",
    "    x = annotate(DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'),quantize_config = activationquant())(x)\n",
    "\n",
    "    x = annotate(Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)), quantize_config = ModifiedDenseQuantizeConfig())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'), quantize_config = activationquant())(x)\n",
    "\n",
    "    # 12th\n",
    "    x = annotate(DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'), quantize_config = activationquant())(x)\n",
    "\n",
    "    x = annotate(Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)), quantize_config = ModifiedDenseQuantizeConfig())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'), quantize_config = activationquant())(x)\n",
    "\n",
    "    # 13th layer, depthwise separable conv\n",
    "    x = annotate(DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'), quantize_config = activationquant())(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = annotate(Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)), quantize_config = ModifiedDenseQuantizeConfig())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'), quantize_config = activationquant())(x)\n",
    "\n",
    "    # 14th layer, depthwise separable conv\n",
    "    x = annotate(DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'), quantize_config = activationquant())(x)\n",
    "\n",
    "    x = annotate(Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4)), quantize_config = ModifiedDenseQuantizeConfig())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = annotate(Activation('relu'), quantize_config = activationquant())(x)\n",
    "\n",
    "    # Average pooling, max polling may be used also\n",
    "    # Keras employs GlobalAveragePooling2D \n",
    "    x = AveragePooling2D(pool_size=x.shape[1:3])(x)\n",
    "    #x = MaxPooling2D(pool_size=x.shape[1:3])(x)\n",
    "\n",
    "    # Keras inserts Dropout() and a pointwise Conv2D() here\n",
    "    # We are staying with the paper base structure\n",
    "\n",
    "    # Flatten, FC layer and classify\n",
    "    x = Flatten()(x)\n",
    "    outputs = annotate(Dense(num_classes, activation='softmax'), quantize_config = ModifiedDenseQuantizeConfig())(x)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd019e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 96, 96, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 48, 48, 8)         224       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 8)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d (DepthwiseC (None, 48, 48, 8)         80        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 16)        144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 48, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 48, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 24, 24, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_2 (Depthwis (None, 24, 24, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 32)        1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_3 (Depthwis (None, 12, 12, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 64)        2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 12, 12, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 64)        4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_5 (Depthwis (None, 6, 6, 64)          640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 128)         8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_6 (Depthwis (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate (QuantizeA (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_1 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_2 (Quantiz (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_3 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_4 (Quantiz (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_5 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_6 (Quantiz (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_7 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_8 (Quantiz (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_9 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_10 (Quanti (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_11 (Quanti (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_12 (Quanti (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_13 (Quanti (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_14 (Quanti (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_15 (Quanti (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_16 (Quanti (None, 3, 3, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_17 (Quanti (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_18 (Quanti (None, 3, 3, 256)         33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "quantize_annotate_19 (Quanti (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_20 (Quanti (None, 3, 3, 256)         2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "quantize_annotate_21 (Quanti (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_22 (Quanti (None, 3, 3, 256)         65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "quantize_annotate_23 (Quanti (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_24 (Quanti (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 221,794\n",
      "Trainable params: 216,322\n",
      "Non-trainable params: 5,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = mobilenet_v1()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9c2cb",
   "metadata": {},
   "source": [
    "## Traninig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fff58031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "\n",
    "\n",
    "IMAGE_SIZE = 96\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "\n",
    "BASE_DIR = os.path.join(os.getcwd(),'C:/Users/abirh/Desktop/codes/Actual_exp/MobilenetV1/p1/vw_coco2014_96')\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    model = mobilenet_v1()\n",
    "    batch_size = 50\n",
    "    validation_split = 0.1\n",
    "\n",
    "    quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "    # `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with `quantize_scope`\n",
    "    # as well as the custom Keras layer.\n",
    "    with quantize_scope({\n",
    "        'ModifiedDenseQuantizeConfig':ModifiedDenseQuantizeConfig,\n",
    "        'activationquant'   :activationquant}):\n",
    "      # Use `quantize_apply` to actually make the model quantization aware.\n",
    "      model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "          rotation_range=10,\n",
    "          width_shift_range=0.05,\n",
    "          height_shift_range=0.05,\n",
    "          zoom_range=.1,\n",
    "          horizontal_flip=True,\n",
    "          validation_split=validation_split,\n",
    "          rescale=1. / 255)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "          BASE_DIR,\n",
    "          target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          subset='training',\n",
    "          color_mode='rgb')\n",
    "    val_generator = datagen.flow_from_directory(\n",
    "          BASE_DIR,\n",
    "          target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          subset='validation',\n",
    "          color_mode='rgb')\n",
    "    print(train_generator.class_indices)\n",
    "\n",
    "    model = train_epochs(model, train_generator, val_generator, 20, 0.001)\n",
    "    model = train_epochs(model, train_generator, val_generator, 10, 0.0005)\n",
    "    model = train_epochs(model, train_generator, val_generator, 20, 0.00025)\n",
    "\n",
    "\n",
    "def train_epochs(model, train_generator, val_generator, epoch_count,\n",
    "                     learning_rate):\n",
    "    model.compile(\n",
    "          optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "    history_fine = model.fit(\n",
    "          train_generator,\n",
    "          steps_per_epoch=len(train_generator),\n",
    "          epochs=epoch_count,\n",
    "          validation_data=val_generator,\n",
    "          validation_steps=len(val_generator),\n",
    "          batch_size=BATCH_SIZE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29830b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98658 images belonging to 2 classes.\n",
      "Found 10961 images belonging to 2 classes.\n",
      "{'non_person': 0, 'person': 1}\n",
      "Epoch 1/20\n",
      "3084/3084 [==============================] - 215s 68ms/step - loss: 0.9141 - accuracy: 0.6047 - val_loss: 0.8455 - val_accuracy: 0.6513\n",
      "Epoch 2/20\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.7686 - accuracy: 0.6655 - val_loss: 0.7428 - val_accuracy: 0.6511\n",
      "Epoch 3/20\n",
      "3084/3084 [==============================] - 199s 65ms/step - loss: 0.6683 - accuracy: 0.6948 - val_loss: 0.6262 - val_accuracy: 0.7144\n",
      "Epoch 4/20\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.6094 - accuracy: 0.7163 - val_loss: 0.5872 - val_accuracy: 0.7337\n",
      "Epoch 5/20\n",
      "3084/3084 [==============================] - 199s 64ms/step - loss: 0.5658 - accuracy: 0.7418 - val_loss: 0.5595 - val_accuracy: 0.7344\n",
      "Epoch 6/20\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.5366 - accuracy: 0.7575 - val_loss: 0.5226 - val_accuracy: 0.7632\n",
      "Epoch 7/20\n",
      "3084/3084 [==============================] - 199s 65ms/step - loss: 0.5149 - accuracy: 0.7701 - val_loss: 0.4921 - val_accuracy: 0.7845\n",
      "Epoch 8/20\n",
      "3084/3084 [==============================] - 199s 64ms/step - loss: 0.4992 - accuracy: 0.7790 - val_loss: 0.4947 - val_accuracy: 0.7746\n",
      "Epoch 9/20\n",
      "3084/3084 [==============================] - 199s 64ms/step - loss: 0.4861 - accuracy: 0.7851 - val_loss: 0.5584 - val_accuracy: 0.7622\n",
      "Epoch 10/20\n",
      "3084/3084 [==============================] - 199s 65ms/step - loss: 0.4751 - accuracy: 0.7915 - val_loss: 0.4707 - val_accuracy: 0.7927\n",
      "Epoch 11/20\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.4694 - accuracy: 0.7949 - val_loss: 0.4586 - val_accuracy: 0.7976\n",
      "Epoch 12/20\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.4596 - accuracy: 0.8002 - val_loss: 0.4515 - val_accuracy: 0.8092\n",
      "Epoch 13/20\n",
      "3084/3084 [==============================] - 199s 64ms/step - loss: 0.4539 - accuracy: 0.8055 - val_loss: 0.4894 - val_accuracy: 0.7890\n",
      "Epoch 14/20\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.4495 - accuracy: 0.8072 - val_loss: 0.5767 - val_accuracy: 0.7549\n",
      "Epoch 15/20\n",
      "3084/3084 [==============================] - 199s 65ms/step - loss: 0.4425 - accuracy: 0.8111 - val_loss: 0.4346 - val_accuracy: 0.8146\n",
      "Epoch 16/20\n",
      "3084/3084 [==============================] - 198s 64ms/step - loss: 0.4401 - accuracy: 0.8130 - val_loss: 0.4393 - val_accuracy: 0.8136\n",
      "Epoch 17/20\n",
      "3084/3084 [==============================] - 199s 64ms/step - loss: 0.4353 - accuracy: 0.8160 - val_loss: 0.4515 - val_accuracy: 0.8004\n",
      "Epoch 18/20\n",
      "3084/3084 [==============================] - 198s 64ms/step - loss: 0.4309 - accuracy: 0.8179 - val_loss: 0.4238 - val_accuracy: 0.8229\n",
      "Epoch 19/20\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.4270 - accuracy: 0.8203 - val_loss: 0.4568 - val_accuracy: 0.8047\n",
      "Epoch 20/20\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.4259 - accuracy: 0.8206 - val_loss: 0.4196 - val_accuracy: 0.8222\n",
      "Epoch 1/10\n",
      "3084/3084 [==============================] - 204s 65ms/step - loss: 0.4070 - accuracy: 0.8305 - val_loss: 0.3924 - val_accuracy: 0.8403\n",
      "Epoch 2/10\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.4019 - accuracy: 0.8332 - val_loss: 0.3897 - val_accuracy: 0.8402\n",
      "Epoch 3/10\n",
      "3084/3084 [==============================] - 201s 65ms/step - loss: 0.3963 - accuracy: 0.8345 - val_loss: 0.3930 - val_accuracy: 0.8338\n",
      "Epoch 4/10\n",
      "3084/3084 [==============================] - 198s 64ms/step - loss: 0.3945 - accuracy: 0.8380 - val_loss: 0.3932 - val_accuracy: 0.8395\n",
      "Epoch 5/10\n",
      "3084/3084 [==============================] - 198s 64ms/step - loss: 0.3917 - accuracy: 0.8373 - val_loss: 0.3870 - val_accuracy: 0.8401\n",
      "Epoch 6/10\n",
      "3084/3084 [==============================] - 199s 65ms/step - loss: 0.3897 - accuracy: 0.8386 - val_loss: 0.3910 - val_accuracy: 0.8376\n",
      "Epoch 7/10\n",
      "3084/3084 [==============================] - 199s 64ms/step - loss: 0.3867 - accuracy: 0.8392 - val_loss: 0.3963 - val_accuracy: 0.8349\n",
      "Epoch 8/10\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.3853 - accuracy: 0.8400 - val_loss: 0.3841 - val_accuracy: 0.8404\n",
      "Epoch 9/10\n",
      "3084/3084 [==============================] - 202s 66ms/step - loss: 0.3830 - accuracy: 0.8433 - val_loss: 0.3728 - val_accuracy: 0.8482\n",
      "Epoch 10/10\n",
      "3084/3084 [==============================] - 201s 65ms/step - loss: 0.3834 - accuracy: 0.8421 - val_loss: 0.3863 - val_accuracy: 0.8422\n",
      "Epoch 1/20\n",
      "3084/3084 [==============================] - 202s 65ms/step - loss: 0.3709 - accuracy: 0.8490 - val_loss: 0.3658 - val_accuracy: 0.8501\n",
      "Epoch 2/20\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.3654 - accuracy: 0.8517 - val_loss: 0.3682 - val_accuracy: 0.8464\n",
      "Epoch 3/20\n",
      "3084/3084 [==============================] - 198s 64ms/step - loss: 0.3618 - accuracy: 0.8528 - val_loss: 0.3657 - val_accuracy: 0.8472\n",
      "Epoch 4/20\n",
      "3084/3084 [==============================] - 201s 65ms/step - loss: 0.3604 - accuracy: 0.8538 - val_loss: 0.3624 - val_accuracy: 0.8531\n",
      "Epoch 5/20\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.3588 - accuracy: 0.8531 - val_loss: 0.3680 - val_accuracy: 0.8453\n",
      "Epoch 6/20\n",
      "3084/3084 [==============================] - 199s 65ms/step - loss: 0.3588 - accuracy: 0.8539 - val_loss: 0.3612 - val_accuracy: 0.8485\n",
      "Epoch 7/20\n",
      "3084/3084 [==============================] - 200s 65ms/step - loss: 0.3554 - accuracy: 0.8549 - val_loss: 0.3609 - val_accuracy: 0.8487\n",
      "Epoch 8/20\n",
      "3084/3084 [==============================] - 199s 64ms/step - loss: 0.3548 - accuracy: 0.8544 - val_loss: 0.3590 - val_accuracy: 0.8478\n",
      "Epoch 9/20\n",
      "3084/3084 [==============================] - 199s 65ms/step - loss: 0.3528 - accuracy: 0.8567 - val_loss: 0.3595 - val_accuracy: 0.8522\n",
      "Epoch 10/20\n",
      "3084/3084 [==============================] - 199s 65ms/step - loss: 0.3513 - accuracy: 0.8563 - val_loss: 0.3629 - val_accuracy: 0.8513\n",
      "Epoch 11/20\n",
      "3084/3084 [==============================] - 199s 65ms/step - loss: 0.3520 - accuracy: 0.8565 - val_loss: 0.3670 - val_accuracy: 0.8501\n",
      "Epoch 12/20\n",
      "3084/3084 [==============================] - 198s 64ms/step - loss: 0.3502 - accuracy: 0.8575 - val_loss: 0.3761 - val_accuracy: 0.8446\n",
      "Epoch 13/20\n",
      "3084/3084 [==============================] - 199s 64ms/step - loss: 0.3487 - accuracy: 0.8574 - val_loss: 0.3547 - val_accuracy: 0.8546\n",
      "Epoch 14/20\n",
      "3084/3084 [==============================] - 199s 64ms/step - loss: 0.3470 - accuracy: 0.8597 - val_loss: 0.3619 - val_accuracy: 0.8532\n",
      "Epoch 15/20\n",
      "3084/3084 [==============================] - 199s 64ms/step - loss: 0.3475 - accuracy: 0.8582 - val_loss: 0.3576 - val_accuracy: 0.8529\n",
      "Epoch 16/20\n",
      "3084/3084 [==============================] - 199s 64ms/step - loss: 0.3467 - accuracy: 0.8581 - val_loss: 0.3559 - val_accuracy: 0.8538\n",
      "Epoch 17/20\n",
      "3084/3084 [==============================] - 197s 64ms/step - loss: 0.3443 - accuracy: 0.8594 - val_loss: 0.3675 - val_accuracy: 0.8507\n",
      "Epoch 18/20\n",
      "3084/3084 [==============================] - 198s 64ms/step - loss: 0.3456 - accuracy: 0.8592 - val_loss: 0.3560 - val_accuracy: 0.8537\n",
      "Epoch 19/20\n",
      "3084/3084 [==============================] - 199s 65ms/step - loss: 0.3431 - accuracy: 0.8601 - val_loss: 0.3550 - val_accuracy: 0.8499\n",
      "Epoch 20/20\n",
      "3084/3084 [==============================] - 199s 65ms/step - loss: 0.3433 - accuracy: 0.8605 - val_loss: 0.3510 - val_accuracy: 0.8600\n"
     ]
    }
   ],
   "source": [
    "train_model() # Comment this once tranined and keep the traning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80c0b5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as activation_15_layer_call_fn, activation_15_layer_call_and_return_conditional_losses, restored_function_body, restored_function_body, activation_16_layer_call_fn while saving (showing 5 of 95). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: P1Q1_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: P1Q1_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('P1Q1_model') # Comment this once tranined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a761329",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('P1Q1_model.h5') # Comment this once tranined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59938835",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44761ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('P1Q1_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c2581d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\abirh\\\\Desktop\\\\codes\\\\Actual_exp\\\\MobilenetV1\\\\p1\\\\p1q1\\\\vw_coco2014_96'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8094134a7c0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m               \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'validation'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m               color_mode='rgb')\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m model.compile(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\smartsight\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[0mfollow_links\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m         interpolation=interpolation)\n\u001b[0m\u001b[0;32m    960\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m   def flow_from_dataframe(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\smartsight\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\smartsight\\lib\\site-packages\\keras_preprocessing\\image\\directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\abirh\\\\Desktop\\\\codes\\\\Actual_exp\\\\MobilenetV1\\\\p1\\\\p1q1\\\\vw_coco2014_96'"
     ]
    }
   ],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "              rotation_range=10,\n",
    "              width_shift_range=0.05,\n",
    "              height_shift_range=0.05,\n",
    "              zoom_range=.1,\n",
    "              horizontal_flip=True,\n",
    "              validation_split=.1,\n",
    "              rescale=1. / 255)\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "              BASE_DIR,\n",
    "              target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "              batch_size=BATCH_SIZE,\n",
    "              subset='validation',\n",
    "              color_mode='rgb')\n",
    "\n",
    "model.compile(\n",
    "              metrics=['acc'])\n",
    "\n",
    "acc = model.evaluate(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb13b69-9081-4ecb-9480-5cdbba467d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 96, 96, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 48, 48, 8)         224       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 8)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d (DepthwiseC (None, 48, 48, 8)         80        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 16)        144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 48, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 48, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 24, 24, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_2 (Depthwis (None, 24, 24, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 32)        1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_3 (Depthwis (None, 12, 12, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 64)        2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 12, 12, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 64)        4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_5 (Depthwis (None, 6, 6, 64)          640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 128)         8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_6 (Depthwis (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate (QuantizeA (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_1 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_2 (Quantiz (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_3 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_4 (Quantiz (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_5 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_6 (Quantiz (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_7 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_8 (Quantiz (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_9 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_10 (Quanti (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_11 (Quanti (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_12 (Quanti (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_13 (Quanti (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_14 (Quanti (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_15 (Quanti (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_16 (Quanti (None, 3, 3, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_17 (Quanti (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_18 (Quanti (None, 3, 3, 256)         33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "quantize_annotate_19 (Quanti (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_20 (Quanti (None, 3, 3, 256)         2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "quantize_annotate_21 (Quanti (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_22 (Quanti (None, 3, 3, 256)         65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "quantize_annotate_23 (Quanti (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_24 (Quanti (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 221,794\n",
      "Trainable params: 216,322\n",
      "Non-trainable params: 5,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "542e593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction index: 0\n",
      "actual image: non-person\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import os\n",
    "\n",
    "\n",
    "# BASE_DIR = os.path.join(os.getcwd(),'vw_coco2014_96')\n",
    "# new_dir = os.path.join(BASE_DIR, 'non_person/COCO_train2014_000000000009.jpg')\n",
    "img = 'C:/Users/abirh/Desktop/codes/Actual_exp/MobilenetV1/p1/vw_coco2014_96/non_person/COCO_train2014_000000000009.jpg'\n",
    "#load the image\n",
    "my_image = load_img(img, target_size=(96, 96, 3))\n",
    "\n",
    "#preprocess the image\n",
    "my_image = img_to_array(my_image)\n",
    "my_image = my_image.reshape((1, my_image.shape[0], my_image.shape[1], my_image.shape[2]))\n",
    "my_image = preprocess_input(my_image)\n",
    "\n",
    "#make the prediction\n",
    "prediction = model.predict(my_image)\n",
    "print(\"prediction index:\",np.argmax(prediction, axis = -1)[0])\n",
    "print(\"actual image: non-person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f628a97",
   "metadata": {},
   "source": [
    "## Partition MCU+Edge\n",
    "firstPart = MCU, lastPart = Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7b1aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Layer should be the last layer before the partition\n",
    "def partition(model, partitionLayer):\n",
    "  num_layers = len(model.layers) - 1\n",
    "  if partitionLayer >= num_layers:\n",
    "    return \"pick a lower partition layer, yours is or is greater than the last layer\"\n",
    "  clientLayers = []\n",
    "  serverLayers = []\n",
    "  for i in range (0,partitionLayer+1):\n",
    "      clientLayers.append(model.get_layer(index=i))\n",
    "  for i in range (partitionLayer+1,len(model.layers)):\n",
    "      serverLayers.append(model.get_layer(index=i))\n",
    "  #Creating Client and Server models\n",
    "  clientModel = tf.keras.Sequential(clientLayers)\n",
    "  clientModel.add(Flatten())\n",
    "  serverModel = tf.keras.Sequential(serverLayers)\n",
    "  print(\"partitioning layer is =\",partitionLayer)\n",
    "  return clientModel, serverModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7ae0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitioning layer is = 45\n"
     ]
    }
   ],
   "source": [
    "partition_layer = 45\n",
    "firstPart, lastPart = partition(model, partition_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed1d1b",
   "metadata": {},
   "source": [
    "## Run image through both partitions to create a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7f5e87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual image index: non-person\n",
      "predicted index: 0\n",
      "(1, 6, 6, 128)\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "# image_num = 0\n",
    "#print(list(image.reshape(1,3072)[0]))\n",
    "first_part_output = firstPart.predict(my_image)\n",
    "\n",
    "new_shape = lastPart.layers[0].input_shape\n",
    "first_part_output = first_part_output.reshape(1,new_shape[1], new_shape[2], new_shape[3])\n",
    "\n",
    "last_part_output = lastPart.predict(first_part_output)\n",
    "output = np.argmax(lastPart.predict(first_part_output))\n",
    "print(\"actual image index: non-person\")\n",
    "print(\"predicted index:\",output)\n",
    "print(first_part_output.shape)\n",
    "# print(first_part_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62ea196c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 8)         224       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 8)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d (DepthwiseC (None, 48, 48, 8)         80        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 16)        144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 48, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 48, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 24, 24, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_2 (Depthwis (None, 24, 24, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 32)        1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_3 (Depthwis (None, 12, 12, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 64)        2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 12, 12, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 64)        4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_5 (Depthwis (None, 6, 6, 64)          640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 128)         8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_6 (Depthwis (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "=================================================================\n",
      "Total params: 39,776\n",
      "Trainable params: 38,144\n",
      "Non-trainable params: 1,632\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "firstPart.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c7ec359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_annotate (QuantizeA (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_1 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_2 (Quantiz (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_3 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_4 (Quantiz (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_5 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_6 (Quantiz (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_7 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_8 (Quantiz (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_9 (Quantiz (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_10 (Quanti (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_11 (Quanti (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_12 (Quanti (None, 6, 6, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_13 (Quanti (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_14 (Quanti (None, 6, 6, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_15 (Quanti (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_16 (Quanti (None, 3, 3, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "quantize_annotate_17 (Quanti (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_18 (Quanti (None, 3, 3, 256)         33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "quantize_annotate_19 (Quanti (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_20 (Quanti (None, 3, 3, 256)         2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "quantize_annotate_21 (Quanti (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_22 (Quanti (None, 3, 3, 256)         65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "quantize_annotate_23 (Quanti (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_24 (Quanti (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 182,018\n",
      "Trainable params: 178,178\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lastPart.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7242d0",
   "metadata": {},
   "source": [
    "## Deploy to MCU \n",
    "#### TFLite and .h model of First Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ec72ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\abirh\\AppData\\Local\\Temp\\tmp8dt649qr\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\abirh\\AppData\\Local\\Temp\\tmp8dt649qr\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68640"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def representative_dataset_gen():\n",
    "        dataset_dir = os.path.join(BASE_DIR, \"person\")\n",
    "        for idx, image_file in enumerate(os.listdir(dataset_dir)):\n",
    "            # 10 representative images should be enough for calibration.\n",
    "            if idx > 10:\n",
    "                return\n",
    "            full_path = os.path.join(dataset_dir, image_file)\n",
    "            if os.path.isfile(full_path):\n",
    "                img = tf.keras.preprocessing.image.load_img(\n",
    "                    full_path, color_mode='rgb').resize((96, 96))\n",
    "                arr = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                # Scale input to [0, 1.0] like in training.\n",
    "                yield [arr.reshape(1, 96, 96, 3) / 255.]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(firstPart)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "tflite_model = converter.convert()\n",
    "open(\"p1q1.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f730137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425639"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import binascii\n",
    "\n",
    "def convert_to_c_array(bytes) -> str:\n",
    "  hexstr = binascii.hexlify(bytes).decode(\"UTF-8\")\n",
    "  hexstr = hexstr.upper()\n",
    "  array = [\"0x\" + hexstr[i:i + 2] for i in range(0, len(hexstr), 2)]\n",
    "  array = [array[i:i+10] for i in range(0, len(array), 10)]\n",
    "  return \",\\n  \".join([\", \".join(e) for e in array])\n",
    "\n",
    "tflite_binary = open(\"p1q1.tflite\", 'rb').read()\n",
    "ascii_bytes = convert_to_c_array(tflite_binary)\n",
    "c_file = \"const unsigned char tf_model[] = {\\n  \" + ascii_bytes + \"\\n};\\nunsigned int tf_model_len = \" + str(len(tflite_binary)) + \";\"\n",
    "# print(c_file)\n",
    "open(\"p1q1.h\", \"w\").write(c_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d24dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
