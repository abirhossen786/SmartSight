{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QAWT_V5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ4TCuZylwJk",
        "outputId": "82830d61-d8f5-4792-e870-1e8c656d4a89"
      },
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install -q tf-nightly\n",
        "!pip install -q tensorflow-model-optimization"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Successfully uninstalled tensorflow-2.4.1\n",
            "\u001b[K     |████████████████████████████████| 452.9MB 38kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9MB 23.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0MB 45.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2MB 45.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 53.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 25.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 52.1MB/s \n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 174kB 18.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbsCspMPmXxb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from keras.datasets import cifar10"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z47R8lyNmfb2"
      },
      "source": [
        "### Loading CIFAR-10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgsVhAFmmZN1",
        "outputId": "f147a805-78a8-4fe2-986e-08c6240e76f3"
      },
      "source": [
        "(trainX, trainy), (testX, testy) = cifar10.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGM3Jr9-mw4v",
        "outputId": "008b1911-013a-4e32-cca8-b385147cb145"
      },
      "source": [
        "# print to make sure we have the correct shapes + number of images for training\n",
        "print(\"number of train pictures:\", trainX.shape)\n",
        "print(\"number of trained picture values:\", trainy.shape)\n",
        "# divide by 255 to make [0,255] into [0,1] + print to make sure!\n",
        "trainy = tf.keras.utils.to_categorical(trainy,10)\n",
        "testy = tf.keras.utils.to_categorical(testy,10)\n",
        "trainX = trainX/255.0\n",
        "testX = testX/255.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of train pictures: (50000, 32, 32, 3)\n",
            "number of trained picture values: (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZMi7dE9nEW3"
      },
      "source": [
        "### vgg16 without quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOHcMQFPnHsY",
        "outputId": "a30219e7-1235-412a-de4f-028b5e424e1a"
      },
      "source": [
        "# Define the model architecture.\n",
        "model_vgg16 = tf.keras.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.Reshape(target_shape=(32, 32, 3)),\n",
        "  #block-1\n",
        "  tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "  #block-2\n",
        "  tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "  #block-3\n",
        "  tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "  #block-4\n",
        "  tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "  #block-5\n",
        "  tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(4096),\n",
        "  tf.keras.layers.Dense(4096),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "model_vgg16.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_11 (Reshape)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_130 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "conv2d_131 (Conv2D)          (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_132 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_133 (Conv2D)          (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_134 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_135 (Conv2D)          (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "conv2d_136 (Conv2D)          (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_52 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_137 (Conv2D)          (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_138 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_139 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_53 (MaxPooling (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_140 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_141 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_142 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_54 (MaxPooling (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 4096)              2101248   \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 10)                40970     \n",
            "=================================================================\n",
            "Total params: 33,638,218\n",
            "Trainable params: 33,638,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN7okvSGw09U"
      },
      "source": [
        "### Training on vanilla vgg-16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxdi_Mh4w_Ze",
        "outputId": "1a5448a6-f265-43ec-93e0-9a12ce428567"
      },
      "source": [
        "# from keras.optimizers import Adam\n",
        "# from keras.optimizers import SGD\n",
        "\n",
        "# Compile the model\n",
        "model_vgg16.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy,metrics=['accuracy'])\n",
        "\n",
        "# Fit data to model\n",
        "model_vgg16.fit(trainX[:200], trainy[:200],\n",
        "          batch_size=50,\n",
        "          epochs=1,\n",
        "          validation_split=0.1,\n",
        "          )"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 14s 3s/step - loss: 2.3186 - accuracy: 0.1500 - val_loss: 2.4663 - val_accuracy: 0.0500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd80729c850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx93S2wvy0vj"
      },
      "source": [
        "### Defining the quantization functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHdAaFQwy6FF"
      },
      "source": [
        "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
        "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
        "\n",
        "num_bits_wights = 16\n",
        "num_bits_activation = 16\n",
        "\n",
        "class LayerQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
        "    # Configure how to quantize weights.\n",
        "    def get_weights_and_quantizers(self, layer):\n",
        "      return [(layer.kernel, LastValueQuantizer(num_bits_wights, symmetric=True, narrow_range=False, per_axis=False))]\n",
        "\n",
        "    # Configure how to quantize activations.\n",
        "    def get_activations_and_quantizers(self, layer):\n",
        "      return [(layer.activation, MovingAverageQuantizer(num_bits_activation, symmetric=False, narrow_range=False, per_axis=False))]\n",
        "\n",
        "    def set_quantize_weights(self, layer, quantize_weights):\n",
        "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
        "      # , in the same order\n",
        "      layer.kernel = quantize_weights[0]\n",
        "\n",
        "    def set_quantize_activations(self, layer, quantize_activations):\n",
        "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
        "      # , in the same order.\n",
        "      layer.activation = quantize_activations[0]\n",
        "\n",
        "    # Configure how to quantize outputs (may be equivalent to activations).\n",
        "    def get_output_quantizers(self, layer):\n",
        "      return []\n",
        "\n",
        "    def get_config(self):\n",
        "      return {}"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6_06CeszcbN"
      },
      "source": [
        "### Quantize vgg16 ALL LAYERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMl_lrSPzb2C",
        "outputId": "2a8ca983-a27d-42ca-efd3-e1a75ef7a034"
      },
      "source": [
        "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
        "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
        "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
        "\n",
        "# Define the model architecture.\n",
        "model_vgg16_quant = tf.keras.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.Reshape(target_shape=(32, 32, 3)),\n",
        "  #block-1\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "  #block-2\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "  #block-3\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),padding=\"same\", activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "  #block-4\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "  #block-5\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  quantize_annotate_layer(tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding=\"same\", activation='relu'), LayerQuantizeConfig()),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  \n",
        "\n",
        "  tf.keras.layers.Flatten(),\n",
        "  quantize_annotate_layer(tf.keras.layers.Dense(4096), LayerQuantizeConfig()),\n",
        "  quantize_annotate_layer(tf.keras.layers.Dense(4096), LayerQuantizeConfig()),\n",
        "  quantize_annotate_layer(tf.keras.layers.Dense(10), LayerQuantizeConfig())\n",
        "])\n",
        "\n",
        "# `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with `quantize_scope`\n",
        "# as well as the custom Keras layer.\n",
        "with quantize_scope(\n",
        "  {'LayerQuantizeConfig': LayerQuantizeConfig}):\n",
        "  # Use `quantize_apply` to actually make the model quantization aware.\n",
        "  quant_aware_model_vgg16 = tfmot.quantization.keras.quantize_apply(model_vgg16_quant)\n",
        "  quant_aware_model_vgg16.summary()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_24 (Reshape)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "quant_conv2d_299 (QuantizeWr (None, 32, 32, 64)        1797      \n",
            "_________________________________________________________________\n",
            "quant_conv2d_300 (QuantizeWr (None, 32, 32, 64)        36933     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_115 (MaxPoolin (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "quant_conv2d_301 (QuantizeWr (None, 16, 16, 128)       73861     \n",
            "_________________________________________________________________\n",
            "quant_conv2d_302 (QuantizeWr (None, 16, 16, 128)       147589    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_116 (MaxPoolin (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "quant_conv2d_303 (QuantizeWr (None, 8, 8, 256)         295173    \n",
            "_________________________________________________________________\n",
            "quant_conv2d_304 (QuantizeWr (None, 8, 8, 256)         590085    \n",
            "_________________________________________________________________\n",
            "conv2d_305 (Conv2D)          (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_117 (MaxPoolin (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "quant_conv2d_306 (QuantizeWr (None, 4, 4, 512)         1180165   \n",
            "_________________________________________________________________\n",
            "quant_conv2d_307 (QuantizeWr (None, 4, 4, 512)         2359813   \n",
            "_________________________________________________________________\n",
            "quant_conv2d_308 (QuantizeWr (None, 4, 4, 512)         2359813   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_118 (MaxPoolin (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "quant_conv2d_309 (QuantizeWr (None, 2, 2, 512)         2359813   \n",
            "_________________________________________________________________\n",
            "quant_conv2d_310 (QuantizeWr (None, 2, 2, 512)         2359813   \n",
            "_________________________________________________________________\n",
            "quant_conv2d_311 (QuantizeWr (None, 2, 2, 512)         2359813   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_119 (MaxPoolin (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_23 (Flatten)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "quant_dense_69 (QuantizeWrap (None, 4096)              2101253   \n",
            "_________________________________________________________________\n",
            "quant_dense_70 (QuantizeWrap (None, 4096)              16781317  \n",
            "_________________________________________________________________\n",
            "quant_dense_71 (QuantizeWrap (None, 10)                40975     \n",
            "=================================================================\n",
            "Total params: 33,638,293\n",
            "Trainable params: 33,638,218\n",
            "Non-trainable params: 75\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-6HCkbk0sda"
      },
      "source": [
        "### Tarining on quantize vgg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qHUhSwh0zDU",
        "outputId": "307651a3-b21e-4871-eeea-fbfbc2ee602c"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "#opt = SGD(learning_rate=0.1)\n",
        "# Compile the model\n",
        "quant_aware_model_vgg16.compile(optimizer='sgd', loss=tf.keras.losses.categorical_crossentropy,metrics=['accuracy'])\n",
        "\n",
        "# Fit data to model\n",
        "quant_aware_model_vgg16.fit(trainX[:200], trainy[:200],\n",
        "          batch_size=50,\n",
        "          epochs=1,\n",
        "          validation_split=0.1,\n",
        "          )\n"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 18s 4s/step - loss: 3.4867 - accuracy: 0.1000 - val_loss: 2.7855 - val_accuracy: 0.0500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd7f9d5a910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOWx-KiX1t13"
      },
      "source": [
        "### Compare baseline vgg16 and quantize vgg16 Train Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56qeswlh13Wr",
        "outputId": "ca63a3fd-a176-4172-f775-dbc605f696b8"
      },
      "source": [
        "data_size = 200\n",
        "\n",
        "baseline_model_accuracy = model_vgg16.evaluate(\n",
        "  trainX[:data_size], trainy[:data_size], verbose=1)\n",
        "\n",
        "q_aware_model_accuracy = quant_aware_model_vgg16.evaluate(\n",
        "  trainX[:data_size], trainy[:data_size], verbose=1)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Quant test accuracy:', q_aware_model_accuracy)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 2s 287ms/step - loss: 2.2978 - accuracy: 0.1400\n",
            "7/7 [==============================] - 2s 313ms/step - loss: 3.4142 - accuracy: 0.0950\n",
            "Baseline test accuracy: [2.2977724075317383, 0.14000000059604645]\n",
            "Quant test accuracy: [3.414229393005371, 0.0949999988079071]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cpS91n1C_aG"
      },
      "source": [
        "### Compare baseline vgg16 and quantize vgg16 Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13Tk9b1NC-7D",
        "outputId": "6c46857c-162c-4813-8e26-62c743cc1956"
      },
      "source": [
        "baseline_model_accuracy = model_vgg16.evaluate(\n",
        "  testX, testy, verbose=1)\n",
        "\n",
        "q_aware_model_accuracy = quant_aware_model_vgg16.evaluate(\n",
        "  testX, testy, verbose=1)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 98s 314ms/step - loss: 2.3548 - accuracy: 0.1000\n",
            "313/313 [==============================] - 108s 346ms/step - loss: 3.4844 - accuracy: 0.1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFvJFp25Gfip"
      },
      "source": [
        "### Converting to tfLite for uploading microntroller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-EdGnhOES-5",
        "outputId": "bcf00e2c-1021-4cbe-d0bd-66e8951785ea"
      },
      "source": [
        "import tempfile\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model_vgg16)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()\n",
        "# Create float TFLite model.\n",
        "float_converter = tf.lite.TFLiteConverter.from_keras_model(model_vgg16)\n",
        "float_tflite_model = float_converter.convert()\n",
        "\n",
        "# Measure sizes of models.\n",
        "_, float_file = tempfile.mkstemp('.tflite')\n",
        "_, quant_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quant_file, 'wb') as f:\n",
        "  f.write(quantized_tflite_model)\n",
        "\n",
        "with open(float_file, 'wb') as f:\n",
        "  f.write(float_tflite_model)\n",
        "\n",
        "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
        "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_299_layer_call_fn, conv2d_299_layer_call_and_return_conditional_losses, conv2d_300_layer_call_fn, conv2d_300_layer_call_and_return_conditional_losses, conv2d_301_layer_call_fn while saving (showing 5 of 75). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpx_un2xb8/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpx_un2xb8/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap8StAHBC7VO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}