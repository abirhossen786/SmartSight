# -*- coding: utf-8 -*-
"""QuantizedVGG3.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZWQ8L_TC2AK-q-CetQnBvfiNTdiPc3mz
"""

#!pip3 install tensorflow
#!pip3 uninstall -y tf-nightly
!pip3 install -q tensorflow-model-optimization

import numpy                         as np
import tensorflow                    as tf
import tensorflow_datasets           as tfds
import matplotlib.pyplot             as plt
import tensorflow_model_optimization as tfmot

from tensorflow.keras                import   datasets,   layers,   models
from tensorflow.keras.optimizers     import   SGD, Adam
from keras.preprocessing.image       import ImageDataGenerator

LastValueQuantizer     = tfmot.quantization.keras.quantizers.LastValueQuantizer
MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer
annotate_layer         = tfmot.quantization.keras.quantize_annotate_layer
annotate_model         = tfmot.quantization.keras.quantize_annotate_model

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

validation_images = train_images[-10000:,:,:,:]
validation_labels = train_labels[-10000:,:]

train_images      = train_images[:-10000,:,:,:]
train_labels      = train_labels[:-10000,:]

train_images      = train_images.astype('float32')/255
test_images       = test_images.astype('float32')/255
validation_images = validation_images.astype('float32')/255

train_labels      = tf.keras.utils.to_categorical(train_labels,10)
test_labels       = tf.keras.utils.to_categorical(test_labels,10)
validation_labels = tf.keras.utils.to_categorical(validation_labels,10)

class VGGHeterogenousNetwork:
    def __init__(self,numBitsOfLayersList,numBitsOfActivationsList):
        class QuantizationConfig1(tfmot.quantization.keras.QuantizeConfig):
            def get_weights_and_quantizers(self, layer):
              return [(layer.kernel, LastValueQuantizer(num_bits=numBitsOfLayersList[0], symmetric=True, narrow_range=False, per_axis=False))]
            def get_activations_and_quantizers(self, layer):
              return [(layer.activation, MovingAverageQuantizer(num_bits=numBitsOfActivationsList[0], symmetric=False, narrow_range=False, per_axis=False))]
            def set_quantize_weights(self, layer, quantize_weights):
              layer.kernel = quantize_weights[0]
            def set_quantize_activations(self, layer, quantize_activations):
              layer.activation = quantize_activations[0]
            def get_output_quantizers(self, layer):
              return []
            def get_config(self):
              return {}

        class QuantizationConfig2(tfmot.quantization.keras.QuantizeConfig):
            def get_weights_and_quantizers(self, layer):
              return [(layer.kernel, LastValueQuantizer(num_bits=numBitsOfLayersList[1], symmetric=True, narrow_range=False, per_axis=False))]
            def get_activations_and_quantizers(self, layer):
              return [(layer.activation, MovingAverageQuantizer(num_bits=numBitsOfActivationsList[1], symmetric=False, narrow_range=False, per_axis=False))]
            def set_quantize_weights(self, layer, quantize_weights):
              layer.kernel = quantize_weights[0]
            def set_quantize_activations(self, layer, quantize_activations):
              layer.activation = quantize_activations[0]
            def get_output_quantizers(self, layer):
              return []
            def get_config(self):
              return {}

        class QuantizationConfig3(tfmot.quantization.keras.QuantizeConfig):
            def get_weights_and_quantizers(self, layer):
              return [(layer.kernel, LastValueQuantizer(num_bits=numBitsOfLayersList[2], symmetric=True, narrow_range=False, per_axis=False))]
            def get_activations_and_quantizers(self, layer):
              return [(layer.activation, MovingAverageQuantizer(num_bits=numBitsOfActivationsList[2], symmetric=False, narrow_range=False, per_axis=False))]
            def set_quantize_weights(self, layer, quantize_weights):
              layer.kernel = quantize_weights[0]
            def set_quantize_activations(self, layer, quantize_activations):
              layer.activation = quantize_activations[0]
            def get_output_quantizers(self, layer):
              return []
            def get_config(self):
              return {}      
        class QuantizationConfig4(tfmot.quantization.keras.QuantizeConfig):
            def get_weights_and_quantizers(self, layer):
              return [(layer.kernel, LastValueQuantizer(num_bits=numBitsOfLayersList[3], symmetric=True, narrow_range=False, per_axis=False))]
            def get_activations_and_quantizers(self, layer):
              return [(layer.activation, MovingAverageQuantizer(num_bits=numBitsOfActivationsList[3], symmetric=False, narrow_range=False, per_axis=False))]
            def set_quantize_weights(self, layer, quantize_weights):
              layer.kernel = quantize_weights[0]
            def set_quantize_activations(self, layer, quantize_activations):
              layer.activation = quantize_activations[0]
            def get_output_quantizers(self, layer):
              return []
            def get_config(self):
              return {}    
        class QuantizationConfig5(tfmot.quantization.keras.QuantizeConfig):
            def get_weights_and_quantizers(self, layer):
              return [(layer.kernel, LastValueQuantizer(num_bits=numBitsOfLayersList[4], symmetric=True, narrow_range=False, per_axis=False))]
            def get_activations_and_quantizers(self, layer):
              return [(layer.activation, MovingAverageQuantizer(num_bits=numBitsOfActivationsList[4], symmetric=False, narrow_range=False, per_axis=False))]
            def set_quantize_weights(self, layer, quantize_weights):
              layer.kernel = quantize_weights[0]
            def set_quantize_activations(self, layer, quantize_activations):
              layer.activation = quantize_activations[0]
            def get_output_quantizers(self, layer):
              return []
            def get_config(self):
              return {}
        class QuantizationConfig6(tfmot.quantization.keras.QuantizeConfig):
            def get_weights_and_quantizers(self, layer):
              return [(layer.kernel, LastValueQuantizer(num_bits=numBitsOfLayersList[5], symmetric=True, narrow_range=False, per_axis=False))]
            def get_activations_and_quantizers(self, layer):
              return [(layer.activation, MovingAverageQuantizer(num_bits=numBitsOfActivationsList[5], symmetric=False, narrow_range=False, per_axis=False))]
            def set_quantize_weights(self, layer, quantize_weights):
              layer.kernel = quantize_weights[0]
            def set_quantize_activations(self, layer, quantize_activations):
              layer.activation = quantize_activations[0]
            def get_output_quantizers(self, layer):
              return []
            def get_config(self):
              return {}    
        class QuantizationConfig7(tfmot.quantization.keras.QuantizeConfig):
            def get_weights_and_quantizers(self, layer):
              return [(layer.kernel, LastValueQuantizer(num_bits=numBitsOfLayersList[6], symmetric=True, narrow_range=False, per_axis=False))]
            def get_activations_and_quantizers(self, layer):
              return [(layer.activation, MovingAverageQuantizer(num_bits=numBitsOfActivationsList[6], symmetric=False, narrow_range=False, per_axis=False))]
            def set_quantize_weights(self, layer, quantize_weights):
              layer.kernel = quantize_weights[0]
            def set_quantize_activations(self, layer, quantize_activations):
              layer.activation = quantize_activations[0]
            def get_output_quantizers(self, layer):
              return []
            def get_config(self):
              return {}              

                                                

        self.qvgg3 = tf.keras.Sequential([
        layers.InputLayer(input_shape=(32, 32, 3)),
        layers.Reshape(target_shape=(32, 32, 3)),
        annotate_layer(layers.Conv2D(input_shape=(32,32,3),
                                     filters=32,kernel_size=(3,3),
                                     padding="same", activation="relu", kernel_initializer='he_uniform',
                                     name='block1_conv1'), quantize_config=QuantizationConfig1()),
        layers.BatchNormalization(),                             
        annotate_layer(layers.Conv2D(filters=32, kernel_size=(3,3), kernel_initializer='he_uniform',
                                     padding="same", activation="relu",
                                     name='block1_conv2'), quantize_config=QuantizationConfig2()),
        layers.BatchNormalization(),                             
        layers.MaxPool2D(pool_size=(2,2), strides=(2,2), name='block1_pool'),
                                        
        layers.Dropout(0.4),

        annotate_layer(layers.Conv2D(filters=64, kernel_size=(3,3), kernel_initializer='he_uniform', 
                                     padding="same", activation="relu",
                                     name='block2_conv1'), quantize_config=QuantizationConfig3()),  
        layers.BatchNormalization(),                             
        annotate_layer(layers.Conv2D(filters=64, kernel_size=(3,3), kernel_initializer='he_uniform',
                                     padding="same", activation="relu",
                                     name='block2_conv2'), quantize_config=QuantizationConfig4()),
        layers.BatchNormalization(),                             
        layers.MaxPool2D(pool_size=(2,2),strides=(2,2), name='block2_pool'),
        layers.Dropout(0.5),

        annotate_layer(layers.Conv2D(filters=128, kernel_size=(3,3), kernel_initializer='he_uniform',
                                     padding="same", activation="relu",
                                     name='block3_conv1'), quantize_config=QuantizationConfig5()),  
        layers.BatchNormalization(),                             
        annotate_layer(layers.Conv2D(filters=128, kernel_size=(3,3), kernel_initializer='he_uniform',
                                     padding="same", activation="relu",
                                     name='block3_conv2'), quantize_config=QuantizationConfig6()),
        layers.BatchNormalization(),                             
        layers.MaxPool2D(pool_size=(2,2),strides=(2,2), name='block3_pool'),
        layers.Dropout(0.6),

        annotate_layer(layers.Flatten(name='flatten')),
        annotate_layer(layers.Dense(units=128, activation='relu', kernel_initializer='he_uniform',name='fc1'), quantize_config=QuantizationConfig7()),
        layers.Dropout(0.7),
        layers.Dense(units=10, activation="softmax",name='predictions') ]  )  

        self.quantize_scope = tfmot.quantization.keras.quantize_scope


        with self.quantize_scope(  {'QuantizationConfig1': QuantizationConfig1,
                                    'QuantizationConfig2': QuantizationConfig2,                
                                    'QuantizationConfig3': QuantizationConfig3,
                                    'QuantizationConfig4': QuantizationConfig4,
                                    'QuantizationConfig5': QuantizationConfig5, 
                                    'QuantizationConfig6': QuantizationConfig6,
                                    'QuantizationConfig7': QuantizationConfig7, }  ):
          
             self.quantizedmodel = tfmot.quantization.keras.quantize_apply(self.qvgg3)

        #opt = tf.keras.optimizers.Adam(learning_rate=0.1)
        opt = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')
        self.quantizedmodel.compile(optimizer=opt,loss=tf.keras.losses.categorical_crossentropy,metrics=['accuracy'])

    def Train(self,Epoch=5,batchSize=32): 
        #datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)
        #it_train = datagen.flow(train_images, train_labels, batch_size=batchSize)
        #steps = int(train_images.shape[0] / batchSize)
 
        self.history = self.quantizedmodel.fit(train_images, train_labels, epochs=Epoch, batch_size=batchSize,verbose=2,validation_split=0.3)
        #self.history = self.quantizedmodel.fit(it_train, steps_per_epoch=steps, epochs=Epoch, batch_size=batchSize,verbose=2,validation_data=(validation_images, validation_labels))

    def Summary(self):
        self.quantizedmodel.summary()

    def Evaluate(self):
        _, acc = self.quantizedmodel.evaluate(test_images, test_labels, verbose=2) 
        return acc

def f(numBitsOfLayersList=[16,16,16,16,16,16,16],numBitsOfActivationsList=[16,16,16,16,16,16,16],Epoch=100,batchSize=64):
    QVGG3 = VGGHeterogenousNetwork(numBitsOfLayersList,numBitsOfActivationsList)
    QVGG3.Train(Epoch=Epoch,batchSize=batchSize)
    print("Test Result: ")
    acc = QVGG3.Evaluate()
    return acc

#f(numBitsOfLayersList=[16,16,16,16,16,16,16],numBitsOfActivationsList=[16,16,16,16,16,16,16],Epoch=100,batchSize=128)
#f(numBitsOfLayersList=[4,4,4,4,4,4,4],numBitsOfActivationsList=[4,4,4,4,4,4,4],Epoch=100,batchSize=128)
#f(numBitsOfLayersList=[2,2,2,2,2,2,2],numBitsOfActivationsList=[2,2,2,2,2,2,2],Epoch=100,batchSize=128)
f(numBitsOfLayersList=[3,3,3,3,3,3,3],numBitsOfActivationsList=[3,3,3,3,3,3,3],Epoch=100,batchSize=128)

f(numBitsOfLayersList=[2,2,2,2,2,2,2],numBitsOfActivationsList=[2,2,2,2,2,2,2],Epoch=100,batchSize=128)

f(numBitsOfLayersList=[3,3,3,3,3,3,3],numBitsOfActivationsList=[2,2,2,2,2,2,2],Epoch=100,batchSize=128)

f(numBitsOfLayersList=[2,2,2,2,2,2,2],numBitsOfActivationsList=[3,3,3,3,3,3,3],Epoch=100,batchSize=128)

f(numBitsOfLayersList=[3,3,3,2,2,2,2],numBitsOfActivationsList=[3,3,3,3,3,3,3],Epoch=100,batchSize=128)

f(numBitsOfLayersList=[2,2,2,3,3,3,3],numBitsOfActivationsList=[3,3,3,3,3,3,3],Epoch=100,batchSize=128)

f(numBitsOfLayersList=[3,3,3,3,3,3,2],numBitsOfActivationsList=[3,3,3,3,3,3,3],Epoch=100,batchSize=128)