{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d770ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10f90dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train pictures: (50000, 32, 32, 3)\n",
      "number of trained picture values: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "# print to make sure we have the correct shapes + number of images for training\n",
    "print(\"number of train pictures:\", trainX.shape)\n",
    "print(\"number of trained picture values:\", trainy.shape)\n",
    "# divide by 255 to make [0,255] into [0,1] + print to make sure!\n",
    "trainy = tf.keras.utils.to_categorical(trainy,10)\n",
    "testy = tf.keras.utils.to_categorical(testy,10)\n",
    "trainX = trainX/255.0\n",
    "testX = testX/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3506b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "class ModifiedDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "      layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "      layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "      return []\n",
    "\n",
    "    def get_config(self):\n",
    "      return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b2667",
   "metadata": {},
   "source": [
    "### Quantizing vgg-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7e1d01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer (QuantizeLaye (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_block1_conv1 (Quantize (None, 32, 32, 64)        1797      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "quant_block1_conv2 (Quantize (None, 32, 32, 64)        36933     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "quant_block2_conv1 (Quantize (None, 16, 16, 128)       73861     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "quant_block2_conv2 (Quantize (None, 16, 16, 128)       147589    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "quant_block3_conv1 (Quantize (None, 8, 8, 256)         295173    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "quant_block3_conv2 (Quantize (None, 8, 8, 256)         590085    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "quant_block3_conv3 (Quantize (None, 8, 8, 256)         590085    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "quant_block4_conv1 (Quantize (None, 4, 4, 512)         1180165   \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block4_conv2 (Quantize (None, 4, 4, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block4_conv3 (Quantize (None, 4, 4, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block5_conv1 (Quantize (None, 2, 2, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block5_conv2 (Quantize (None, 2, 2, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block5_conv3 (Quantize (None, 2, 2, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_flatten (QuantizeWrapp (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "quant_fc1 (QuantizeWrapper)  (None, 512)               262661    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 14,999,444\n",
      "Trainable params: 14,990,922\n",
      "Non-trainable params: 8,522\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "annotate = tfmot.quantization.keras.quantize_annotate_layer\n",
    "\n",
    "quant_vgg16 = tf.keras.Sequential()\n",
    "    # Only annotated layers will be quantized\n",
    "    \n",
    "#block-1\n",
    "quant_vgg16.add(annotate(Conv2D(input_shape=(32,32,3),\n",
    "                    filters=64,kernel_size=(3,3),\n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block1_conv1'), quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.3))   \n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=64,\n",
    "                    kernel_size=(3,3),\n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block1_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2), strides=(2,2), name='block1_pool'))\n",
    "\n",
    "\n",
    "#block-2\n",
    "quant_vgg16.add(annotate(Conv2D(filters=128, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block2_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=128, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block2_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block2_pool'))\n",
    "\n",
    "#block-3\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block3_pool'))\n",
    "\n",
    "#block-4\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block4_pool'))\n",
    "\n",
    "#block-5\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block5_pool'))\n",
    "\n",
    "#fc1, fc2 and predictions\n",
    "quant_vgg16.add(Dropout(0.5))\n",
    "quant_vgg16.add(annotate(Flatten(name='flatten')))\n",
    "quant_vgg16.add(annotate(Dense(units=512,activation=\"relu\",name='fc1'), quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(Dropout(0.5))\n",
    "quant_vgg16.add(Dense(units=10, activation=\"softmax\",name='predictions'))    \n",
    "  \n",
    "\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "# `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with `quantize_scope`\n",
    "# as well as the custom Keras layer.\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig':ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  vgg_quant_model = tfmot.quantization.keras.quantize_apply(quant_vgg16)\n",
    "    \n",
    "vgg_quant_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9411b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 19s 19ms/step - loss: 2.4709 - accuracy: 0.1472 - val_loss: 2.5450 - val_accuracy: 0.1058\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.1276 - accuracy: 0.1933 - val_loss: 2.7973 - val_accuracy: 0.1202\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.2015 - accuracy: 0.1767 - val_loss: 3.0607 - val_accuracy: 0.1038\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.1505 - accuracy: 0.1823 - val_loss: 3.6451 - val_accuracy: 0.1019\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.0494 - accuracy: 0.2053 - val_loss: 3.6542 - val_accuracy: 0.1027\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.9976 - accuracy: 0.2194 - val_loss: 3.1324 - val_accuracy: 0.1060\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.9596 - accuracy: 0.2344 - val_loss: 3.5248 - val_accuracy: 0.1024\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.9257 - accuracy: 0.2466 - val_loss: 3.5735 - val_accuracy: 0.1055\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.8972 - accuracy: 0.2585 - val_loss: 3.3090 - val_accuracy: 0.1112\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.8942 - accuracy: 0.2601 - val_loss: 3.4700 - val_accuracy: 0.1170\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.8559 - accuracy: 0.2741 - val_loss: 3.6971 - val_accuracy: 0.1129\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.8215 - accuracy: 0.2873 - val_loss: 3.8089 - val_accuracy: 0.1173\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.7946 - accuracy: 0.2973 - val_loss: 3.3056 - val_accuracy: 0.1295\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.7839 - accuracy: 0.3047 - val_loss: 2.7991 - val_accuracy: 0.1718\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.7467 - accuracy: 0.3194 - val_loss: 3.2356 - val_accuracy: 0.1753\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.7155 - accuracy: 0.3368 - val_loss: 4.0371 - val_accuracy: 0.1476\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.6739 - accuracy: 0.3564 - val_loss: 3.9481 - val_accuracy: 0.1609\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.6378 - accuracy: 0.3774 - val_loss: 5.6770 - val_accuracy: 0.1582\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.6169 - accuracy: 0.3877 - val_loss: 3.6784 - val_accuracy: 0.1859\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.5838 - accuracy: 0.4090 - val_loss: 2.5795 - val_accuracy: 0.1926\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.7519 - accuracy: 0.3446 - val_loss: 2.2832 - val_accuracy: 0.1275\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.9655 - accuracy: 0.2473 - val_loss: 6.1085 - val_accuracy: 0.0990\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.9241 - accuracy: 0.2664 - val_loss: 2.4766 - val_accuracy: 0.1342\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.8128 - accuracy: 0.3009 - val_loss: 2.1076 - val_accuracy: 0.2049\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.7222 - accuracy: 0.3403 - val_loss: 2.5694 - val_accuracy: 0.1839\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.6677 - accuracy: 0.3626 - val_loss: 3.3604 - val_accuracy: 0.1666\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.6386 - accuracy: 0.3790 - val_loss: 3.0703 - val_accuracy: 0.1765\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.6119 - accuracy: 0.3896 - val_loss: 3.2585 - val_accuracy: 0.2060\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.5920 - accuracy: 0.4021 - val_loss: 2.5745 - val_accuracy: 0.2002\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.5338 - accuracy: 0.4329 - val_loss: 2.6891 - val_accuracy: 0.2603\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.5010 - accuracy: 0.4476 - val_loss: 2.8353 - val_accuracy: 0.2238\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.6283 - accuracy: 0.3958 - val_loss: 2.5999 - val_accuracy: 0.2345\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.4949 - accuracy: 0.4499 - val_loss: 4.1145 - val_accuracy: 0.2008\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.4468 - accuracy: 0.4709 - val_loss: 4.6583 - val_accuracy: 0.1726\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.4051 - accuracy: 0.4899 - val_loss: 2.1961 - val_accuracy: 0.3205\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.3769 - accuracy: 0.5071 - val_loss: 2.6327 - val_accuracy: 0.2548\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.3454 - accuracy: 0.5200 - val_loss: 2.6526 - val_accuracy: 0.2579\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.3047 - accuracy: 0.5390 - val_loss: 2.7440 - val_accuracy: 0.2565\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.2668 - accuracy: 0.5541 - val_loss: 2.6783 - val_accuracy: 0.2429\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.2315 - accuracy: 0.5719 - val_loss: 2.6422 - val_accuracy: 0.2506\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.1968 - accuracy: 0.5847 - val_loss: 2.9600 - val_accuracy: 0.2230\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.1628 - accuracy: 0.5971 - val_loss: 2.9851 - val_accuracy: 0.2228\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.1436 - accuracy: 0.6054 - val_loss: 2.9578 - val_accuracy: 0.2309\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.1092 - accuracy: 0.6166 - val_loss: 2.8341 - val_accuracy: 0.2517\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.0834 - accuracy: 0.6306 - val_loss: 2.9451 - val_accuracy: 0.2146\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.0701 - accuracy: 0.6374 - val_loss: 2.7890 - val_accuracy: 0.2592\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.0478 - accuracy: 0.6460 - val_loss: 2.9750 - val_accuracy: 0.2574\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.0128 - accuracy: 0.6565 - val_loss: 2.9863 - val_accuracy: 0.2439\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.9955 - accuracy: 0.6647 - val_loss: 2.8369 - val_accuracy: 0.2202\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.9723 - accuracy: 0.6737 - val_loss: 2.7667 - val_accuracy: 0.2850\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.9460 - accuracy: 0.6795 - val_loss: 2.9430 - val_accuracy: 0.2505\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.9266 - accuracy: 0.6896 - val_loss: 2.8107 - val_accuracy: 0.2639\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.9132 - accuracy: 0.6950 - val_loss: 2.4954 - val_accuracy: 0.3399\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.8936 - accuracy: 0.7024 - val_loss: 3.1311 - val_accuracy: 0.2810\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.8736 - accuracy: 0.7071 - val_loss: 2.6379 - val_accuracy: 0.3178\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.8558 - accuracy: 0.7155 - val_loss: 2.5574 - val_accuracy: 0.3117\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 15s 19ms/step - loss: 0.8429 - accuracy: 0.7165 - val_loss: 2.9117 - val_accuracy: 0.2900\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.8211 - accuracy: 0.7296 - val_loss: 2.0459 - val_accuracy: 0.4020\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.8079 - accuracy: 0.7350 - val_loss: 2.0349 - val_accuracy: 0.4189\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.7964 - accuracy: 0.7364 - val_loss: 2.3364 - val_accuracy: 0.3566\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.7758 - accuracy: 0.7445 - val_loss: 2.2529 - val_accuracy: 0.3749\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.7642 - accuracy: 0.7487 - val_loss: 2.4145 - val_accuracy: 0.3757\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.7652 - accuracy: 0.7494 - val_loss: 2.0597 - val_accuracy: 0.4291\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.7287 - accuracy: 0.7625 - val_loss: 2.0345 - val_accuracy: 0.4308\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.7173 - accuracy: 0.7662 - val_loss: 1.9696 - val_accuracy: 0.4616\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.7071 - accuracy: 0.7720 - val_loss: 2.5278 - val_accuracy: 0.3854\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6907 - accuracy: 0.7730 - val_loss: 2.4089 - val_accuracy: 0.3470\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6828 - accuracy: 0.7779 - val_loss: 2.0763 - val_accuracy: 0.4403\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6616 - accuracy: 0.7857 - val_loss: 2.3479 - val_accuracy: 0.4180\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6505 - accuracy: 0.7885 - val_loss: 2.0409 - val_accuracy: 0.4536\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6435 - accuracy: 0.7910 - val_loss: 2.1775 - val_accuracy: 0.4460\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6515 - accuracy: 0.7881 - val_loss: 2.7434 - val_accuracy: 0.3286\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.7120 - accuracy: 0.7699 - val_loss: 1.9002 - val_accuracy: 0.4834\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6393 - accuracy: 0.7920 - val_loss: 2.4326 - val_accuracy: 0.4214\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6236 - accuracy: 0.7983 - val_loss: 2.1470 - val_accuracy: 0.4375\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6011 - accuracy: 0.8039 - val_loss: 1.8147 - val_accuracy: 0.4982\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5923 - accuracy: 0.8099 - val_loss: 1.9620 - val_accuracy: 0.4580\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5741 - accuracy: 0.8137 - val_loss: 2.2499 - val_accuracy: 0.4414\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5615 - accuracy: 0.8154 - val_loss: 1.8283 - val_accuracy: 0.4924\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5480 - accuracy: 0.8224 - val_loss: 2.0034 - val_accuracy: 0.4920\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5476 - accuracy: 0.8233 - val_loss: 2.2680 - val_accuracy: 0.4062\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5283 - accuracy: 0.8288 - val_loss: 1.7845 - val_accuracy: 0.5401\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5227 - accuracy: 0.8299 - val_loss: 1.8922 - val_accuracy: 0.5167\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5117 - accuracy: 0.8357 - val_loss: 1.6165 - val_accuracy: 0.5843\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4992 - accuracy: 0.8375 - val_loss: 1.9432 - val_accuracy: 0.5259\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5603 - accuracy: 0.8186 - val_loss: 1.7924 - val_accuracy: 0.5385\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5341 - accuracy: 0.8257 - val_loss: 1.6269 - val_accuracy: 0.5756\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5026 - accuracy: 0.8352 - val_loss: 2.0040 - val_accuracy: 0.5110\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4777 - accuracy: 0.8454 - val_loss: 2.1903 - val_accuracy: 0.5349\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4781 - accuracy: 0.8432 - val_loss: 2.1991 - val_accuracy: 0.4948\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4660 - accuracy: 0.8479 - val_loss: 1.5749 - val_accuracy: 0.6213\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4554 - accuracy: 0.8520 - val_loss: 2.3335 - val_accuracy: 0.4800\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4478 - accuracy: 0.8541 - val_loss: 1.7605 - val_accuracy: 0.5549\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4400 - accuracy: 0.8588 - val_loss: 2.1714 - val_accuracy: 0.4963\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4286 - accuracy: 0.8622 - val_loss: 2.2399 - val_accuracy: 0.5083\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4276 - accuracy: 0.8617 - val_loss: 2.0520 - val_accuracy: 0.5651\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4160 - accuracy: 0.8650 - val_loss: 1.8832 - val_accuracy: 0.5541\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4125 - accuracy: 0.8662 - val_loss: 2.1029 - val_accuracy: 0.5402\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3991 - accuracy: 0.8723 - val_loss: 2.1783 - val_accuracy: 0.5257\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4010 - accuracy: 0.8712 - val_loss: 2.1000 - val_accuracy: 0.5195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6bac1b11d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "opt = SGD(learning_rate=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# Compile the model\n",
    "vgg_quant_model.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy,metrics=['accuracy'])\n",
    "\n",
    "# Fit data to model\n",
    "vgg_quant_model.fit(trainX, trainy,\n",
    "          batch_size=50,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6ab40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_quant_model.save_weights('cifar10vgg_quant8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd3949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 2.0701 - accuracy: 0.5220\n",
      "Test loss 2.0701, accuracy 52.20%\n"
     ]
    }
   ],
   "source": [
    "score = vgg_quant_model.evaluate(testX, testy, verbose=1)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "52"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
