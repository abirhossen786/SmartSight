{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d770ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10f90dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train pictures: (50000, 32, 32, 3)\n",
      "number of trained picture values: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "# print to make sure we have the correct shapes + number of images for training\n",
    "print(\"number of train pictures:\", trainX.shape)\n",
    "print(\"number of trained picture values:\", trainy.shape)\n",
    "# divide by 255 to make [0,255] into [0,1] + print to make sure!\n",
    "trainy = tf.keras.utils.to_categorical(trainy,10)\n",
    "testy = tf.keras.utils.to_categorical(testy,10)\n",
    "trainX = trainX/255.0\n",
    "testX = testX/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3506b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "class ModifiedDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.kernel, LastValueQuantizer(num_bits=4, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      return [(layer.activation, MovingAverageQuantizer(num_bits=4, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "      layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "      layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "      return []\n",
    "\n",
    "    def get_config(self):\n",
    "      return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b2667",
   "metadata": {},
   "source": [
    "### Quantizing vgg-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7e1d01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer (QuantizeLaye (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_block1_conv1 (Quantize (None, 32, 32, 64)        1797      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "quant_block1_conv2 (Quantize (None, 32, 32, 64)        36933     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "quant_block2_conv1 (Quantize (None, 16, 16, 128)       73861     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "quant_block2_conv2 (Quantize (None, 16, 16, 128)       147589    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "quant_block3_conv1 (Quantize (None, 8, 8, 256)         295173    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "quant_block3_conv2 (Quantize (None, 8, 8, 256)         590085    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "quant_block3_conv3 (Quantize (None, 8, 8, 256)         590085    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "quant_block4_conv1 (Quantize (None, 4, 4, 512)         1180165   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block4_conv2 (Quantize (None, 4, 4, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block4_conv3 (Quantize (None, 4, 4, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block5_conv1 (Quantize (None, 2, 2, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block5_conv2 (Quantize (None, 2, 2, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block5_conv3 (Quantize (None, 2, 2, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_flatten (QuantizeWrapp (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "quant_fc1 (QuantizeWrapper)  (None, 512)               262661    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 14,999,444\n",
      "Trainable params: 14,990,922\n",
      "Non-trainable params: 8,522\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "annotate = tfmot.quantization.keras.quantize_annotate_layer\n",
    "\n",
    "quant_vgg16 = tf.keras.Sequential()\n",
    "    # Only annotated layers will be quantized\n",
    "    \n",
    "#block-1\n",
    "quant_vgg16.add(annotate(Conv2D(input_shape=(32,32,3),\n",
    "                    filters=64,kernel_size=(3,3),\n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block1_conv1'), quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.3))   \n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=64,\n",
    "                    kernel_size=(3,3),\n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block1_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2), strides=(2,2), name='block1_pool'))\n",
    "\n",
    "\n",
    "#block-2\n",
    "quant_vgg16.add(annotate(Conv2D(filters=128, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block2_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=128, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block2_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block2_pool'))\n",
    "\n",
    "#block-3\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block3_pool'))\n",
    "\n",
    "#block-4\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block4_pool'))\n",
    "\n",
    "#block-5\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block5_pool'))\n",
    "\n",
    "#fc1, fc2 and predictions\n",
    "quant_vgg16.add(Dropout(0.5))\n",
    "quant_vgg16.add(annotate(Flatten(name='flatten')))\n",
    "quant_vgg16.add(annotate(Dense(units=512,activation=\"relu\",name='fc1'), quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(Dropout(0.5))\n",
    "quant_vgg16.add(Dense(units=10, activation=\"softmax\",name='predictions'))    \n",
    "  \n",
    "\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "# `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with `quantize_scope`\n",
    "# as well as the custom Keras layer.\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig':ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  vgg_quant_model = tfmot.quantization.keras.quantize_apply(quant_vgg16)\n",
    "    \n",
    "vgg_quant_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9411b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 19s 20ms/step - loss: 2.5963 - accuracy: 0.0996 - val_loss: 2.3225 - val_accuracy: 0.1014\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3373 - accuracy: 0.1009 - val_loss: 2.3284 - val_accuracy: 0.0952\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3340 - accuracy: 0.0992 - val_loss: 2.3119 - val_accuracy: 0.0997\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3308 - accuracy: 0.1018 - val_loss: 2.3191 - val_accuracy: 0.0977\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3312 - accuracy: 0.0997 - val_loss: 2.3118 - val_accuracy: 0.1022\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3300 - accuracy: 0.1000 - val_loss: 2.3283 - val_accuracy: 0.1003\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3291 - accuracy: 0.0996 - val_loss: 2.3179 - val_accuracy: 0.1016\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3283 - accuracy: 0.0999 - val_loss: 2.3194 - val_accuracy: 0.0997\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3293 - accuracy: 0.1006 - val_loss: 2.3162 - val_accuracy: 0.0997\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3284 - accuracy: 0.1000 - val_loss: 2.3120 - val_accuracy: 0.1014\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3286 - accuracy: 0.0976 - val_loss: 2.3214 - val_accuracy: 0.1014\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3292 - accuracy: 0.1010 - val_loss: 2.3146 - val_accuracy: 0.0997\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3281 - accuracy: 0.1002 - val_loss: 2.3128 - val_accuracy: 0.1014\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3281 - accuracy: 0.1011 - val_loss: 2.3276 - val_accuracy: 0.1014\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3278 - accuracy: 0.0991 - val_loss: 2.3253 - val_accuracy: 0.0952\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3278 - accuracy: 0.1001 - val_loss: 2.3126 - val_accuracy: 0.1016\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3296 - accuracy: 0.0993 - val_loss: 2.3092 - val_accuracy: 0.1014\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3276 - accuracy: 0.0993 - val_loss: 2.3092 - val_accuracy: 0.0952\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3270 - accuracy: 0.1007 - val_loss: 2.3134 - val_accuracy: 0.1014\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3283 - accuracy: 0.0985 - val_loss: 2.3217 - val_accuracy: 0.0980\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3273 - accuracy: 0.0966 - val_loss: 2.3231 - val_accuracy: 0.0952\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3269 - accuracy: 0.0976 - val_loss: 2.3088 - val_accuracy: 0.1014\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3274 - accuracy: 0.0996 - val_loss: 2.3093 - val_accuracy: 0.1014\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3276 - accuracy: 0.1007 - val_loss: 2.3126 - val_accuracy: 0.1025\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3261 - accuracy: 0.0986 - val_loss: 2.3241 - val_accuracy: 0.1014\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3270 - accuracy: 0.0991 - val_loss: 2.3190 - val_accuracy: 0.0980\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3275 - accuracy: 0.1001 - val_loss: 2.3110 - val_accuracy: 0.0980\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3284 - accuracy: 0.0962 - val_loss: 2.3070 - val_accuracy: 0.0997\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3264 - accuracy: 0.0989 - val_loss: 2.3122 - val_accuracy: 0.1003\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3289 - accuracy: 0.0997 - val_loss: 2.3071 - val_accuracy: 0.1003\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3286 - accuracy: 0.0969 - val_loss: 2.3146 - val_accuracy: 0.1025\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3279 - accuracy: 0.0969 - val_loss: 2.3221 - val_accuracy: 0.0980\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3261 - accuracy: 0.0994 - val_loss: 2.3156 - val_accuracy: 0.1022\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3275 - accuracy: 0.0980 - val_loss: 2.3081 - val_accuracy: 0.1014\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3271 - accuracy: 0.0991 - val_loss: 2.3091 - val_accuracy: 0.1003\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3282 - accuracy: 0.0986 - val_loss: 2.3199 - val_accuracy: 0.0977\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3270 - accuracy: 0.0981 - val_loss: 2.3086 - val_accuracy: 0.1025\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3270 - accuracy: 0.1015 - val_loss: 2.3117 - val_accuracy: 0.1022\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3276 - accuracy: 0.1016 - val_loss: 2.3153 - val_accuracy: 0.1025\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3287 - accuracy: 0.0981 - val_loss: 2.3132 - val_accuracy: 0.1014\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3271 - accuracy: 0.0996 - val_loss: 2.3320 - val_accuracy: 0.1016\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3276 - accuracy: 0.1006 - val_loss: 2.3169 - val_accuracy: 0.1014\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3268 - accuracy: 0.1029 - val_loss: 2.3212 - val_accuracy: 0.1014\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3281 - accuracy: 0.0991 - val_loss: 2.3103 - val_accuracy: 0.0980\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3265 - accuracy: 0.0999 - val_loss: 2.3083 - val_accuracy: 0.1016\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3266 - accuracy: 0.1010 - val_loss: 2.3146 - val_accuracy: 0.1014\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3266 - accuracy: 0.1024 - val_loss: 2.3117 - val_accuracy: 0.1014\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3270 - accuracy: 0.1006 - val_loss: 2.3067 - val_accuracy: 0.0980\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3252 - accuracy: 0.1002 - val_loss: 2.3150 - val_accuracy: 0.0952\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3269 - accuracy: 0.0992 - val_loss: 2.3100 - val_accuracy: 0.0952\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3259 - accuracy: 0.0974 - val_loss: 2.3084 - val_accuracy: 0.1014\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3263 - accuracy: 0.0988 - val_loss: 2.3183 - val_accuracy: 0.1016\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3267 - accuracy: 0.0983 - val_loss: 2.3123 - val_accuracy: 0.1014\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3264 - accuracy: 0.0976 - val_loss: 2.3172 - val_accuracy: 0.1014\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3263 - accuracy: 0.0979 - val_loss: 2.3159 - val_accuracy: 0.1014\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3256 - accuracy: 0.0988 - val_loss: 2.3208 - val_accuracy: 0.1016\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3275 - accuracy: 0.1003 - val_loss: 2.3118 - val_accuracy: 0.0952\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3263 - accuracy: 0.1016 - val_loss: 2.3054 - val_accuracy: 0.0980\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3255 - accuracy: 0.0997 - val_loss: 2.3142 - val_accuracy: 0.0997\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3260 - accuracy: 0.0979 - val_loss: 2.3183 - val_accuracy: 0.0980\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3256 - accuracy: 0.0988 - val_loss: 2.3261 - val_accuracy: 0.0980\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3253 - accuracy: 0.1010 - val_loss: 2.3226 - val_accuracy: 0.1025\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3265 - accuracy: 0.1003 - val_loss: 2.3143 - val_accuracy: 0.0980\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3259 - accuracy: 0.0998 - val_loss: 2.3157 - val_accuracy: 0.1025\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3256 - accuracy: 0.0997 - val_loss: 2.3171 - val_accuracy: 0.0952\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3266 - accuracy: 0.1011 - val_loss: 2.3083 - val_accuracy: 0.0980\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3255 - accuracy: 0.0997 - val_loss: 2.3167 - val_accuracy: 0.1025\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3258 - accuracy: 0.0992 - val_loss: 2.3157 - val_accuracy: 0.1014\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3257 - accuracy: 0.0971 - val_loss: 2.3134 - val_accuracy: 0.0977\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3249 - accuracy: 0.0997 - val_loss: 2.3202 - val_accuracy: 0.0977\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3240 - accuracy: 0.1011 - val_loss: 2.3179 - val_accuracy: 0.1022\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3255 - accuracy: 0.1005 - val_loss: 2.3125 - val_accuracy: 0.0997\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3264 - accuracy: 0.1010 - val_loss: 2.3141 - val_accuracy: 0.1022\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3256 - accuracy: 0.0986 - val_loss: 2.3092 - val_accuracy: 0.0977\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3263 - accuracy: 0.0992 - val_loss: 2.3073 - val_accuracy: 0.1003\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3244 - accuracy: 0.1007 - val_loss: 2.3201 - val_accuracy: 0.0952\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3261 - accuracy: 0.0986 - val_loss: 2.3122 - val_accuracy: 0.1016\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3259 - accuracy: 0.1003 - val_loss: 2.3100 - val_accuracy: 0.1003\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3264 - accuracy: 0.1001 - val_loss: 2.3204 - val_accuracy: 0.1014\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3257 - accuracy: 0.0985 - val_loss: 2.3095 - val_accuracy: 0.1016\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3247 - accuracy: 0.1020 - val_loss: 2.3135 - val_accuracy: 0.1022\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3257 - accuracy: 0.1010 - val_loss: 2.3107 - val_accuracy: 0.1022\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3250 - accuracy: 0.1006 - val_loss: 2.3125 - val_accuracy: 0.0980\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3242 - accuracy: 0.1004 - val_loss: 2.3160 - val_accuracy: 0.0977\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3255 - accuracy: 0.1014 - val_loss: 2.3169 - val_accuracy: 0.1025\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3255 - accuracy: 0.1032 - val_loss: 2.3227 - val_accuracy: 0.1014\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3240 - accuracy: 0.1015 - val_loss: 2.3096 - val_accuracy: 0.1025\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3254 - accuracy: 0.1002 - val_loss: 2.3148 - val_accuracy: 0.0977\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3251 - accuracy: 0.0991 - val_loss: 2.3081 - val_accuracy: 0.1025\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3255 - accuracy: 0.0988 - val_loss: 2.3074 - val_accuracy: 0.1016\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3247 - accuracy: 0.0991 - val_loss: 2.3142 - val_accuracy: 0.1025\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3250 - accuracy: 0.1002 - val_loss: 2.3146 - val_accuracy: 0.0980\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3259 - accuracy: 0.1023 - val_loss: 2.3099 - val_accuracy: 0.0997\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3259 - accuracy: 0.1005 - val_loss: 2.3084 - val_accuracy: 0.1025\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3247 - accuracy: 0.0998 - val_loss: 2.3118 - val_accuracy: 0.0952\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3251 - accuracy: 0.0998 - val_loss: 2.3155 - val_accuracy: 0.0977\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3246 - accuracy: 0.0987 - val_loss: 2.3220 - val_accuracy: 0.1025\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3256 - accuracy: 0.1036 - val_loss: 2.3193 - val_accuracy: 0.0997\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3256 - accuracy: 0.0977 - val_loss: 2.3146 - val_accuracy: 0.1014\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.3240 - accuracy: 0.1015 - val_loss: 2.3222 - val_accuracy: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f85fc2e7090>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "opt = SGD(learning_rate=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# Compile the model\n",
    "vgg_quant_model.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy,metrics=['accuracy'])\n",
    "\n",
    "# Fit data to model\n",
    "vgg_quant_model.fit(trainX, trainy,\n",
    "          batch_size=50,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6ab40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_quant_model.save_weights('cifar10vgg_quant4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd3949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 2.3199 - accuracy: 0.1000\n",
      "Test loss 2.3199, accuracy 10.00%\n"
     ]
    }
   ],
   "source": [
    "score = vgg_quant_model.evaluate(testX, testy, verbose=1)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "52"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
