{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d770ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10f90dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train pictures: (50000, 32, 32, 3)\n",
      "number of trained picture values: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "# print to make sure we have the correct shapes + number of images for training\n",
    "print(\"number of train pictures:\", trainX.shape)\n",
    "print(\"number of trained picture values:\", trainy.shape)\n",
    "# divide by 255 to make [0,255] into [0,1] + print to make sure!\n",
    "trainy = tf.keras.utils.to_categorical(trainy,10)\n",
    "testy = tf.keras.utils.to_categorical(testy,10)\n",
    "trainX = trainX/255.0\n",
    "testX = testX/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc883c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-53a99abf1f43>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-53a99abf1f43>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    `DefaultDenseQuantizeConfig` is 8 bit\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Defining the quantization config\n",
    "`DefaultDenseQuantizeConfig` is 8 bit\n",
    "\n",
    "`ModifiedDenseQuantizeConfig` is 4 bit\n",
    "\n",
    "`UltraDenseQuantizeConfig` is 2 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3506b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "class ModifiedDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.kernel, LastValueQuantizer(num_bits=16, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      return [(layer.activation, MovingAverageQuantizer(num_bits=16, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "      layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "      layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "      return []\n",
    "\n",
    "    def get_config(self):\n",
    "      return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e70f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quantizing vgg-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7e1d01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer (QuantizeLaye (None, 32, 32, 3)         3         \n",
      "_________________________________________________________________\n",
      "quant_block1_conv1 (Quantize (None, 32, 32, 64)        1797      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "quant_block1_conv2 (Quantize (None, 32, 32, 64)        36933     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "quant_block2_conv1 (Quantize (None, 16, 16, 128)       73861     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "quant_block2_conv2 (Quantize (None, 16, 16, 128)       147589    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "quant_block3_conv1 (Quantize (None, 8, 8, 256)         295173    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "quant_block3_conv2 (Quantize (None, 8, 8, 256)         590085    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "quant_block3_conv3 (Quantize (None, 8, 8, 256)         590085    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "quant_block4_conv1 (Quantize (None, 4, 4, 512)         1180165   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block4_conv2 (Quantize (None, 4, 4, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block4_conv3 (Quantize (None, 4, 4, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block5_conv1 (Quantize (None, 2, 2, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block5_conv2 (Quantize (None, 2, 2, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_block5_conv3 (Quantize (None, 2, 2, 512)         2359813   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "quant_flatten (QuantizeWrapp (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "quant_fc1 (QuantizeWrapper)  (None, 512)               262661    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 14,999,444\n",
      "Trainable params: 14,990,922\n",
      "Non-trainable params: 8,522\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "annotate = tfmot.quantization.keras.quantize_annotate_layer\n",
    "\n",
    "quant_vgg16 = tf.keras.Sequential()\n",
    "    # Only annotated layers will be quantized\n",
    "    \n",
    "#block-1\n",
    "quant_vgg16.add(annotate(Conv2D(input_shape=(32,32,3),\n",
    "                    filters=64,kernel_size=(3,3),\n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block1_conv1'), quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.3))   \n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=64,\n",
    "                    kernel_size=(3,3),\n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block1_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2), strides=(2,2), name='block1_pool'))\n",
    "\n",
    "\n",
    "#block-2\n",
    "quant_vgg16.add(annotate(Conv2D(filters=128, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block2_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=128, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block2_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block2_pool'))\n",
    "\n",
    "#block-3\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block3_pool'))\n",
    "\n",
    "#block-4\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block4_pool'))\n",
    "\n",
    "#block-5\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block5_pool'))\n",
    "\n",
    "#fc1, fc2 and predictions\n",
    "quant_vgg16.add(Dropout(0.5))\n",
    "quant_vgg16.add(annotate(Flatten(name='flatten')))\n",
    "quant_vgg16.add(annotate(Dense(units=512,activation=\"relu\",name='fc1'), quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(BatchNormalization())\n",
    "\n",
    "quant_vgg16.add(Dropout(0.5))\n",
    "quant_vgg16.add(Dense(units=10, activation=\"softmax\",name='predictions'))    \n",
    "  \n",
    "\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "# `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with `quantize_scope`\n",
    "# as well as the custom Keras layer.\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig':ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  vgg_quant_model = tfmot.quantization.keras.quantize_apply(quant_vgg16)\n",
    "    \n",
    "vgg_quant_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9411b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 19s 20ms/step - loss: 2.4330 - accuracy: 0.1352 - val_loss: 2.6628 - val_accuracy: 0.0956\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 2.0444 - accuracy: 0.2093 - val_loss: 3.4315 - val_accuracy: 0.0949\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.9066 - accuracy: 0.2491 - val_loss: 3.2724 - val_accuracy: 0.0896\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.8247 - accuracy: 0.2800 - val_loss: 3.4548 - val_accuracy: 0.1013\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.7691 - accuracy: 0.3050 - val_loss: 3.2624 - val_accuracy: 0.1177\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.7125 - accuracy: 0.3257 - val_loss: 3.7103 - val_accuracy: 0.0687\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.6801 - accuracy: 0.3416 - val_loss: 4.3152 - val_accuracy: 0.0812\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.6499 - accuracy: 0.3599 - val_loss: 3.3585 - val_accuracy: 0.1112\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.6094 - accuracy: 0.3795 - val_loss: 3.0561 - val_accuracy: 0.1365\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.5706 - accuracy: 0.3983 - val_loss: 4.7717 - val_accuracy: 0.1179\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.5560 - accuracy: 0.4114 - val_loss: 4.4727 - val_accuracy: 0.1339\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 1.5006 - accuracy: 0.4360 - val_loss: 4.3161 - val_accuracy: 0.1199\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.4828 - accuracy: 0.4500 - val_loss: 5.2052 - val_accuracy: 0.1254\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.4250 - accuracy: 0.4760 - val_loss: 4.8761 - val_accuracy: 0.1229\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.3950 - accuracy: 0.4923 - val_loss: 3.6824 - val_accuracy: 0.1631\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.3381 - accuracy: 0.5140 - val_loss: 3.9983 - val_accuracy: 0.1680\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.3292 - accuracy: 0.5192 - val_loss: 2.7442 - val_accuracy: 0.2506\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.2842 - accuracy: 0.5358 - val_loss: 4.8181 - val_accuracy: 0.1542\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.2316 - accuracy: 0.5581 - val_loss: 4.8495 - val_accuracy: 0.1754\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.2206 - accuracy: 0.5671 - val_loss: 4.0199 - val_accuracy: 0.1711\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 1.1890 - accuracy: 0.5786 - val_loss: 3.4020 - val_accuracy: 0.2386\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 1.3539 - accuracy: 0.5114 - val_loss: 2.7253 - val_accuracy: 0.3169\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.1970 - accuracy: 0.5726 - val_loss: 3.7485 - val_accuracy: 0.2241\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.1557 - accuracy: 0.5922 - val_loss: 2.8214 - val_accuracy: 0.3180\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.1197 - accuracy: 0.6124 - val_loss: 3.2363 - val_accuracy: 0.2756\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 1.0880 - accuracy: 0.6247 - val_loss: 2.6627 - val_accuracy: 0.3723\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.0690 - accuracy: 0.6384 - val_loss: 3.3171 - val_accuracy: 0.2722\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 1.0330 - accuracy: 0.6489 - val_loss: 3.0215 - val_accuracy: 0.3197\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.9983 - accuracy: 0.6645 - val_loss: 2.2067 - val_accuracy: 0.3851\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.9542 - accuracy: 0.6791 - val_loss: 2.8135 - val_accuracy: 0.2995\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.9242 - accuracy: 0.6946 - val_loss: 2.1334 - val_accuracy: 0.4438\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.9000 - accuracy: 0.7017 - val_loss: 2.9166 - val_accuracy: 0.3629\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.8675 - accuracy: 0.7106 - val_loss: 2.8193 - val_accuracy: 0.3891\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.8326 - accuracy: 0.7283 - val_loss: 2.7504 - val_accuracy: 0.3958\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.8153 - accuracy: 0.7355 - val_loss: 3.0551 - val_accuracy: 0.3792\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.7907 - accuracy: 0.7420 - val_loss: 1.9594 - val_accuracy: 0.5084\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.7592 - accuracy: 0.7545 - val_loss: 2.4305 - val_accuracy: 0.4569\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.7383 - accuracy: 0.7619 - val_loss: 2.1943 - val_accuracy: 0.4917\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.7236 - accuracy: 0.7677 - val_loss: 3.0815 - val_accuracy: 0.3664\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.6968 - accuracy: 0.7747 - val_loss: 1.5956 - val_accuracy: 0.5580\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.6785 - accuracy: 0.7812 - val_loss: 1.9465 - val_accuracy: 0.5090\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6637 - accuracy: 0.7865 - val_loss: 2.0133 - val_accuracy: 0.4870\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.6421 - accuracy: 0.7917 - val_loss: 2.1850 - val_accuracy: 0.4866\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6301 - accuracy: 0.7961 - val_loss: 2.5030 - val_accuracy: 0.4548\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.6231 - accuracy: 0.8024 - val_loss: 1.8184 - val_accuracy: 0.5426\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.6047 - accuracy: 0.8066 - val_loss: 2.8192 - val_accuracy: 0.4189\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.5997 - accuracy: 0.8066 - val_loss: 1.9948 - val_accuracy: 0.5493\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.5777 - accuracy: 0.8155 - val_loss: 2.1036 - val_accuracy: 0.5198\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.5661 - accuracy: 0.8209 - val_loss: 2.3998 - val_accuracy: 0.4798\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.5499 - accuracy: 0.8257 - val_loss: 2.5906 - val_accuracy: 0.4648\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.5454 - accuracy: 0.8271 - val_loss: 2.1638 - val_accuracy: 0.5110\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.5231 - accuracy: 0.8345 - val_loss: 1.8798 - val_accuracy: 0.5441\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.5145 - accuracy: 0.8345 - val_loss: 2.3349 - val_accuracy: 0.4776\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.5134 - accuracy: 0.8352 - val_loss: 1.9960 - val_accuracy: 0.5565\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.4857 - accuracy: 0.8439 - val_loss: 2.1858 - val_accuracy: 0.5325\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.4804 - accuracy: 0.8468 - val_loss: 1.2751 - val_accuracy: 0.6682\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4844 - accuracy: 0.8450 - val_loss: 2.0673 - val_accuracy: 0.5376\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4658 - accuracy: 0.8517 - val_loss: 1.8761 - val_accuracy: 0.5690\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4518 - accuracy: 0.8573 - val_loss: 1.3809 - val_accuracy: 0.6412\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.4471 - accuracy: 0.8562 - val_loss: 1.5820 - val_accuracy: 0.6276\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4225 - accuracy: 0.8647 - val_loss: 1.6755 - val_accuracy: 0.6111\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4213 - accuracy: 0.8655 - val_loss: 2.0767 - val_accuracy: 0.5513\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4106 - accuracy: 0.8680 - val_loss: 1.4444 - val_accuracy: 0.6386\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.4009 - accuracy: 0.8737 - val_loss: 1.9087 - val_accuracy: 0.5639\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.3960 - accuracy: 0.8743 - val_loss: 1.3302 - val_accuracy: 0.6736\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3917 - accuracy: 0.8747 - val_loss: 1.8925 - val_accuracy: 0.5777\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.3799 - accuracy: 0.8813 - val_loss: 2.3371 - val_accuracy: 0.4962\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3783 - accuracy: 0.8801 - val_loss: 1.8550 - val_accuracy: 0.5768\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3654 - accuracy: 0.8854 - val_loss: 1.4190 - val_accuracy: 0.6716\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.3708 - accuracy: 0.8807 - val_loss: 2.1050 - val_accuracy: 0.5721\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3548 - accuracy: 0.8884 - val_loss: 1.5042 - val_accuracy: 0.6589\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3466 - accuracy: 0.8899 - val_loss: 1.9123 - val_accuracy: 0.6013\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.3468 - accuracy: 0.8901 - val_loss: 1.3113 - val_accuracy: 0.6920\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.3375 - accuracy: 0.8934 - val_loss: 2.2271 - val_accuracy: 0.5456\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.3245 - accuracy: 0.8970 - val_loss: 1.8439 - val_accuracy: 0.6093\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.3215 - accuracy: 0.8977 - val_loss: 1.9113 - val_accuracy: 0.5894\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.3087 - accuracy: 0.9027 - val_loss: 1.6909 - val_accuracy: 0.6396\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.3075 - accuracy: 0.9040 - val_loss: 2.2972 - val_accuracy: 0.5890\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.2928 - accuracy: 0.9077 - val_loss: 2.3143 - val_accuracy: 0.5774\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.2960 - accuracy: 0.9090 - val_loss: 2.0201 - val_accuracy: 0.6128\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.2904 - accuracy: 0.9097 - val_loss: 1.9103 - val_accuracy: 0.6238\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.2948 - accuracy: 0.9071 - val_loss: 1.7772 - val_accuracy: 0.6162\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.2807 - accuracy: 0.9096 - val_loss: 2.5293 - val_accuracy: 0.5298\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.2823 - accuracy: 0.9105 - val_loss: 2.7167 - val_accuracy: 0.5302\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.2739 - accuracy: 0.9136 - val_loss: 1.5339 - val_accuracy: 0.6631\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.2671 - accuracy: 0.9154 - val_loss: 1.8910 - val_accuracy: 0.6270\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.2685 - accuracy: 0.9147 - val_loss: 1.6114 - val_accuracy: 0.6350\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.2530 - accuracy: 0.9205 - val_loss: 1.7264 - val_accuracy: 0.6368\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.2421 - accuracy: 0.9241 - val_loss: 2.3761 - val_accuracy: 0.5717\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.2531 - accuracy: 0.9208 - val_loss: 1.5894 - val_accuracy: 0.6656\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.2481 - accuracy: 0.9207 - val_loss: 1.9377 - val_accuracy: 0.6152\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.2346 - accuracy: 0.9275 - val_loss: 1.5885 - val_accuracy: 0.6507\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 16s 20ms/step - loss: 0.2346 - accuracy: 0.9250 - val_loss: 1.9177 - val_accuracy: 0.6043\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.2354 - accuracy: 0.9272 - val_loss: 2.1171 - val_accuracy: 0.5873\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.2243 - accuracy: 0.9294 - val_loss: 1.9184 - val_accuracy: 0.6332\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.2251 - accuracy: 0.9289 - val_loss: 2.2955 - val_accuracy: 0.5762\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.2116 - accuracy: 0.9360 - val_loss: 3.1302 - val_accuracy: 0.4812\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.2205 - accuracy: 0.9311 - val_loss: 2.4452 - val_accuracy: 0.5458\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.2132 - accuracy: 0.9335 - val_loss: 1.8577 - val_accuracy: 0.6383\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 16s 19ms/step - loss: 0.1998 - accuracy: 0.9376 - val_loss: 1.7032 - val_accuracy: 0.6543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f722c5b2190>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "opt = SGD(learning_rate=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# Compile the model\n",
    "vgg_quant_model.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy,metrics=['accuracy'])\n",
    "\n",
    "# Fit data to model\n",
    "vgg_quant_model.fit(trainX, trainy,\n",
    "          batch_size=50,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6ab40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_quant_model.save_weights('cifar10vgg_quant16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd3949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 1.7137 - accuracy: 0.6501\n",
      "Test loss 1.7137, accuracy 65.01%\n"
     ]
    }
   ],
   "source": [
    "score = vgg_quant_model.evaluate(testX, testy, verbose=1)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
