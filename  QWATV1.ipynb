{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00984624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization import *\n",
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cca8d7",
   "metadata": {},
   "source": [
    "### Loading CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c89c9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainy), (testX, testy) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcc32a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train pictures: (50000, 32, 32, 3)\n",
      "number of trained picture values: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# print to make sure we have the correct shapes + number of images for training\n",
    "print(\"number of train pictures:\", trainX.shape)\n",
    "print(\"number of trained picture values:\", trainy.shape)\n",
    "# divide by 255 to make [0,255] into [0,1] + print to make sure!\n",
    "trainy = tf.keras.utils.to_categorical(trainy,10)\n",
    "testy = tf.keras.utils.to_categorical(testy,10)\n",
    "trainX = trainX/255.0\n",
    "testX = testX/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0b461",
   "metadata": {},
   "source": [
    "### VGG-16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de03c3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# example of loading the vgg16 model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# load model\n",
    "model = VGG16()\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232c249",
   "metadata": {},
   "source": [
    "### VGG-16 Clone Without Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e552e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 33,638,218\n",
      "Trainable params: 33,638,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_wq = tf.keras.Sequential()\n",
    "#block-1\n",
    "model_wq.add(Conv2D(input_shape=(32,32,3),\n",
    "                    filters=64,kernel_size=(3,3),\n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block1_conv1'))\n",
    "model_wq.add(Dropout(0.3))\n",
    "model_wq.add(Conv2D(filters=64,\n",
    "                    kernel_size=(3,3),\n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block1_conv2'))\n",
    "model_wq.add(Dropout(0.4))\n",
    "model_wq.add(MaxPool2D(pool_size=(2,2), strides=(2,2), name='block1_pool'))\n",
    "\n",
    "\n",
    "#block-2\n",
    "model_wq.add(Conv2D(filters=128, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block2_conv1'))\n",
    "model_wq.add(Dropout(0.4))\n",
    "model_wq.add(Conv2D(filters=128, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block2_conv2'))\n",
    "model_wq.add(Dropout(0.4))\n",
    "model_wq.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block2_pool'))\n",
    "\n",
    "#block-3\n",
    "model_wq.add(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv1'))\n",
    "model_wq.add(Dropout(0.4))\n",
    "model_wq.add(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv2'))\n",
    "model_wq.add(Dropout(0.4))\n",
    "model_wq.add(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv3'))\n",
    "model_wq.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block3_pool'))\n",
    "\n",
    "#block-4\n",
    "model_wq.add(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv1'))\n",
    "model_wq.add(Dropout(0.4))\n",
    "model_wq.add(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv2'))\n",
    "model_wq.add(Dropout(0.4))\n",
    "model_wq.add(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv3'))\n",
    "model_wq.add(Dropout(0.4))\n",
    "model_wq.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block4_pool'))\n",
    "\n",
    "#block-5\n",
    "model_wq.add(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv1'))\n",
    "model_wq.add(Dropout(0.4))\n",
    "model_wq.add(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv2'))\n",
    "model_wq.add(Dropout(0.4))\n",
    "model_wq.add(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv3'))\n",
    "model_wq.add(Dropout(0.3))\n",
    "model_wq.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block5_pool'))\n",
    "\n",
    "\n",
    "#fc1, fc2 and predictions\n",
    "model_wq.add(Dropout(0.5))\n",
    "model_wq.add(Flatten(name='flatten'))\n",
    "model_wq.add(Dense(units=4096,activation=\"relu\",name='fc1'))\n",
    "model_wq.add(Dense(units=4096,activation=\"relu\",name='fc2'))\n",
    "\n",
    "model_wq.add(Dropout(0.5))\n",
    "model_wq.add(Dense(units=10, activation=\"softmax\",name='predictions'))\n",
    "\n",
    "model_wq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad691a",
   "metadata": {},
   "source": [
    "### Tranining withgout quantization vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94751e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "16/16 [==============================] - 10s 586ms/step - loss: 2.3001 - accuracy: 0.1146 - val_loss: 2.3074 - val_accuracy: 0.1150\n",
      "Epoch 2/15\n",
      "16/16 [==============================] - 9s 595ms/step - loss: 2.2991 - accuracy: 0.1034 - val_loss: 2.3088 - val_accuracy: 0.1150\n",
      "Epoch 3/15\n",
      "16/16 [==============================] - 9s 585ms/step - loss: 2.2970 - accuracy: 0.1059 - val_loss: 2.3104 - val_accuracy: 0.1150\n",
      "Epoch 4/15\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 2.3034 - accuracy: 0.1033 - val_loss: 2.3115 - val_accuracy: 0.1150\n",
      "Epoch 5/15\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 2.2938 - accuracy: 0.0968 - val_loss: 2.3120 - val_accuracy: 0.1150\n",
      "Epoch 6/15\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 2.3025 - accuracy: 0.1002 - val_loss: 2.3128 - val_accuracy: 0.1150\n",
      "Epoch 7/15\n",
      "16/16 [==============================] - 10s 662ms/step - loss: 2.3024 - accuracy: 0.1200 - val_loss: 2.3133 - val_accuracy: 0.1150\n",
      "Epoch 8/15\n",
      "16/16 [==============================] - 10s 602ms/step - loss: 2.3001 - accuracy: 0.1255 - val_loss: 2.3137 - val_accuracy: 0.1150\n",
      "Epoch 9/15\n",
      "16/16 [==============================] - 10s 641ms/step - loss: 2.2960 - accuracy: 0.1280 - val_loss: 2.3138 - val_accuracy: 0.1150\n",
      "Epoch 10/15\n",
      "16/16 [==============================] - 10s 612ms/step - loss: 2.2938 - accuracy: 0.1162 - val_loss: 2.3143 - val_accuracy: 0.0700\n",
      "Epoch 11/15\n",
      "16/16 [==============================] - 10s 624ms/step - loss: 2.2947 - accuracy: 0.1024 - val_loss: 2.3142 - val_accuracy: 0.1150\n",
      "Epoch 12/15\n",
      "16/16 [==============================] - 10s 612ms/step - loss: 2.3014 - accuracy: 0.1223 - val_loss: 2.3142 - val_accuracy: 0.1150\n",
      "Epoch 13/15\n",
      "16/16 [==============================] - 9s 572ms/step - loss: 2.2991 - accuracy: 0.1153 - val_loss: 2.3140 - val_accuracy: 0.1150\n",
      "Epoch 14/15\n",
      "16/16 [==============================] - 9s 581ms/step - loss: 2.2929 - accuracy: 0.1339 - val_loss: 2.3138 - val_accuracy: 0.1150\n",
      "Epoch 15/15\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 2.2982 - accuracy: 0.1012 - val_loss: 2.3140 - val_accuracy: 0.0700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f92a8ac3730>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "opt = SGD(lr=0.1)\n",
    "# Compile the model\n",
    "model_wq.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy,metrics=['accuracy'])\n",
    "\n",
    "# Fit data to model\n",
    "model_wq.fit(trainX[:1000], trainy[:1000],\n",
    "          batch_size=50,\n",
    "          epochs=15,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882a379",
   "metadata": {},
   "source": [
    "### Loss and Accuracy without quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e17ba6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 33s 106ms/step - loss: 2.3074 - accuracy: 0.1000\n",
      "Test loss 2.3074, accuracy 10.00%\n"
     ]
    }
   ],
   "source": [
    "score = model_wq.evaluate(testX, testy, verbose=1)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3831d6",
   "metadata": {},
   "source": [
    "### Defining the quantization config\n",
    "`DefaultDenseQuantizeConfig` is 8 bit\n",
    "\n",
    "`ModifiedDenseQuantizeConfig` is 4 bit\n",
    "\n",
    "`UltraDenseQuantizeConfig` is 2 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34210b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "class DefaultDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    # Configure how to quantize weights.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    # Configure how to quantize activations.\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "      layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "      layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "      return []\n",
    "\n",
    "    def get_config(self):\n",
    "      return {}\n",
    "\n",
    "class ModifiedDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.kernel, LastValueQuantizer(num_bits=4, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      return [(layer.activation, MovingAverageQuantizer(num_bits=4, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "      layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "      layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "      return []\n",
    "\n",
    "    def get_config(self):\n",
    "      return {}\n",
    "\n",
    "class UltraDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.kernel, LastValueQuantizer(num_bits=2, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      return [(layer.activation, MovingAverageQuantizer(num_bits=2, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "      layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "      layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "      return []\n",
    "\n",
    "    def get_config(self):\n",
    "      return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb47bca",
   "metadata": {},
   "source": [
    "### Quantizing vgg-16\n",
    "`ModifiedDenseQuantizeConfig` is 4 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5676246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7f92736f3970> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7f92736f3970> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7f92736f3970> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x7f92736f3970> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method QuantizeWrapper.call of <tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper.QuantizeWrapper object at 0x7f9273fbbd60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method QuantizeWrapper.call of <tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper.QuantizeWrapper object at 0x7f9273fbbd60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_annotate (QuantizeA (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_1 (Quantiz (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_2 (Quantiz (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_3 (Quantiz (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_4 (Quantiz (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_5 (Quantiz (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_6 (Quantiz (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_7 (Quantiz (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_8 (Quantiz (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_9 (Quantiz (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_10 (Quanti (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_11 (Quanti (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_12 (Quanti (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_13 (Quanti (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_14 (Quanti (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_15 (Quanti (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 33,638,218\n",
      "Trainable params: 33,638,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "annotate = tfmot.quantization.keras.quantize_annotate_layer\n",
    "\n",
    "quant_vgg16 = tf.keras.Sequential()\n",
    "    # Only annotated layers will be quantized\n",
    "    \n",
    "#block-1\n",
    "quant_vgg16.add(annotate(Conv2D(input_shape=(32,32,3),\n",
    "                    filters=64,kernel_size=(3,3),\n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block1_conv1'), quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.3))   \n",
    "\n",
    "quant_vgg16.add(annotate(Conv2D(filters=64,\n",
    "                    kernel_size=(3,3),\n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block1_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2), strides=(2,2), name='block1_pool'))\n",
    "\n",
    "\n",
    "#block-2\n",
    "quant_vgg16.add(annotate(Conv2D(filters=128, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block2_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(annotate(Conv2D(filters=128, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block2_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block2_pool'))\n",
    "\n",
    "#block-3\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(annotate(Conv2D(filters=256, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block3_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block3_pool'))\n",
    "\n",
    "#block-4\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block4_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block4_pool'))\n",
    "\n",
    "#block-5\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv1'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv2'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(annotate(Conv2D(filters=512, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding=\"same\", \n",
    "                    activation=\"relu\",\n",
    "                    name='block5_conv3'),\n",
    "                    quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dropout(0.4))\n",
    "quant_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='block5_pool'))\n",
    "\n",
    "#fc1, fc2 and predictions\n",
    "quant_vgg16.add(Dropout(0.5))\n",
    "quant_vgg16.add(annotate(Flatten(name='flatten')))\n",
    "quant_vgg16.add(annotate(Dense(units=4096,activation=\"relu\",name='fc1'), quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "\n",
    "quant_vgg16.add(Dropout(0.5))\n",
    "quant_vgg16.add(annotate(Dense(units=4096,activation=\"relu\",name='fc2'), quantize_config=ModifiedDenseQuantizeConfig()))\n",
    "quant_vgg16.add(Dense(units=10, activation=\"softmax\",name='predictions'))    \n",
    "  \n",
    "\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "# `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with `quantize_scope`\n",
    "# as well as the custom Keras layer.\n",
    "with quantize_scope(\n",
    "  {'DefaultDenseQuantizeConfig': DefaultDenseQuantizeConfig,\n",
    "  'ModifiedDenseQuantizeConfig':ModifiedDenseQuantizeConfig,\n",
    "  'UltraDenseQuantizeConfig':UltraDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  vgg_quant_model = tfmot.quantization.keras.quantize_apply(quant_vgg16)\n",
    "    \n",
    "quant_vgg16.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37be9251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "16/16 [==============================] - 11s 619ms/step - loss: 2.2952 - accuracy: 0.1073 - val_loss: 2.3047 - val_accuracy: 0.1150\n",
      "Epoch 2/15\n",
      "16/16 [==============================] - 9s 584ms/step - loss: 2.2890 - accuracy: 0.1317 - val_loss: 2.3031 - val_accuracy: 0.1150\n",
      "Epoch 3/15\n",
      "16/16 [==============================] - 9s 585ms/step - loss: 2.2859 - accuracy: 0.1224 - val_loss: 2.3019 - val_accuracy: 0.1150\n",
      "Epoch 4/15\n",
      "16/16 [==============================] - 10s 617ms/step - loss: 2.2790 - accuracy: 0.1483 - val_loss: 2.3014 - val_accuracy: 0.1150\n",
      "Epoch 5/15\n",
      "16/16 [==============================] - 10s 650ms/step - loss: 2.2631 - accuracy: 0.1329 - val_loss: 2.3017 - val_accuracy: 0.1150\n",
      "Epoch 6/15\n",
      "16/16 [==============================] - 10s 620ms/step - loss: 2.2525 - accuracy: 0.1586 - val_loss: 2.3006 - val_accuracy: 0.1150\n",
      "Epoch 7/15\n",
      "16/16 [==============================] - 10s 625ms/step - loss: 2.3197 - accuracy: 0.0985 - val_loss: 2.3032 - val_accuracy: 0.1150\n",
      "Epoch 8/15\n",
      "16/16 [==============================] - 10s 634ms/step - loss: 2.3046 - accuracy: 0.0943 - val_loss: 2.3052 - val_accuracy: 0.1150\n",
      "Epoch 9/15\n",
      "16/16 [==============================] - 10s 598ms/step - loss: 2.3009 - accuracy: 0.1112 - val_loss: 2.3068 - val_accuracy: 0.1150\n",
      "Epoch 10/15\n",
      "16/16 [==============================] - 10s 609ms/step - loss: 2.2986 - accuracy: 0.0973 - val_loss: 2.3074 - val_accuracy: 0.1150\n",
      "Epoch 11/15\n",
      "16/16 [==============================] - 9s 593ms/step - loss: 2.2966 - accuracy: 0.1180 - val_loss: 2.3077 - val_accuracy: 0.1150\n",
      "Epoch 12/15\n",
      "16/16 [==============================] - 10s 610ms/step - loss: 2.2798 - accuracy: 0.1257 - val_loss: 2.3076 - val_accuracy: 0.1150\n",
      "Epoch 13/15\n",
      "16/16 [==============================] - 10s 598ms/step - loss: 2.2979 - accuracy: 0.1202 - val_loss: 2.3088 - val_accuracy: 0.1150\n",
      "Epoch 14/15\n",
      "16/16 [==============================] - 9s 589ms/step - loss: 2.2901 - accuracy: 0.1080 - val_loss: 2.3080 - val_accuracy: 0.1150\n",
      "Epoch 15/15\n",
      "16/16 [==============================] - 10s 614ms/step - loss: 2.2839 - accuracy: 0.1305 - val_loss: 2.3067 - val_accuracy: 0.1150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as block1_conv1_layer_call_and_return_conditional_losses, block1_conv1_layer_call_fn, block1_conv2_layer_call_and_return_conditional_losses, block1_conv2_layer_call_fn, block2_conv1_layer_call_and_return_conditional_losses while saving (showing 5 of 80). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as block1_conv1_layer_call_and_return_conditional_losses, block1_conv1_layer_call_fn, block1_conv2_layer_call_and_return_conditional_losses, block1_conv2_layer_call_fn, block2_conv1_layer_call_and_return_conditional_losses while saving (showing 5 of 80). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpw0_dsveq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpw0_dsveq/assets\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "opt = SGD(lr=0.1)\n",
    "# Compile the model\n",
    "quant_vgg16.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy,metrics=['accuracy'])\n",
    "\n",
    "# Fit data to model\n",
    "quant_vgg16.fit(trainX[:1000], trainy[:1000],\n",
    "          batch_size=50,\n",
    "          epochs=15,\n",
    "          validation_split=0.2)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_vgg16)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448e0cb",
   "metadata": {},
   "source": [
    "### Loss and Accuracy with quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de6c7bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 32s 104ms/step - loss: 2.3063 - accuracy: 0.1000\n",
      "Test loss 2.3063, accuracy 10.00%\n"
     ]
    }
   ],
   "source": [
    "score = quant_vgg16.evaluate(testX, testy, verbose=1)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae5ebd0",
   "metadata": {},
   "source": [
    "### Quantization vs Without quantization Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a3ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
